{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ensemble.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "yrSoPfTryVkc",
        "-BRqQst90P41",
        "ufcTiFmn0T4w",
        "EU30u4LImNY6",
        "fcE9_URBdXOp",
        "gf74ZmIjdAeQ",
        "FWWpcmPBEDty"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dzautriet/bert/blob/master/Ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7_QoQtq4S3m",
        "colab_type": "text"
      },
      "source": [
        "# Set up Kaggle API and download data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzJOjIlF4Tml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6OzURAebLj5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDvtcCRu4PFs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g0PIMWar7nC6",
        "colab": {}
      },
      "source": [
        "!kaggle competitions download -c datasciencebowl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLeIPWL47ppb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip datasciencebowl.zip\n",
        "!rm datasciencebowl.zip\n",
        "!unzip sampleSubmission.csv.zip\n",
        "!rm sampleSubmission.csv.zip\n",
        "!unzip train.zip\n",
        "!rm train.zip\n",
        "!unzip test.zip\n",
        "!rm test.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dO2J2M0_K3O",
        "colab_type": "text"
      },
      "source": [
        "# Import modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FB8OV_Oq_KAT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gc\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Subset, ConcatDataset\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.optim import lr_scheduler\n",
        "from torchvision import models\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phcZoPwUYn2i",
        "colab_type": "code",
        "outputId": "ad917509-061a-47dd-d3b9-f28e7375d76d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Apr  3 17:32:18 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   77C    P0    31W /  70W |   3675MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Pq9Mg0v_e90",
        "colab_type": "text"
      },
      "source": [
        "# Create dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym2beneG_eHo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Colab\n",
        "train_path = 'train'\n",
        "test_path = 'test'\n",
        "csv_path = '.'\n",
        "# save_path = 'drive/My Drive/UCL/applied_machine_learning/Plankton'\n",
        "save_path = 'drive/My Drive/Plankton'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivMHhF5JgNke",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def myloader(image_path, size=96):\n",
        "    \"\"\"\n",
        "    Load image; pad to square shape; resize\n",
        "    \"\"\"\n",
        "    img = Image.open(image_path)\n",
        "    w, h = img.size\n",
        "    if w > h:\n",
        "        to_pad = math.ceil((w - h) / 2)\n",
        "        img = transforms.functional.pad(img, padding=(0, to_pad), fill=(255,))\n",
        "    elif w < h:\n",
        "        to_pad = math.ceil((h - w) / 2)\n",
        "        img = transforms.functional.pad(img, padding=(0, to_pad), fill=(255,))\n",
        "    else:\n",
        "        pass\n",
        "    img = transforms.functional.resize(img, size=(size, size))\n",
        "    return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fklXaZyFhdXL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data augmentation\n",
        "transforms_nonaug = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.9583,), std=(0.1405,))\n",
        "    ])\n",
        "transforms_aug_1 = transforms.Compose([\n",
        "        transforms.RandomRotation((-180, 180), fill=(255,)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.9583,), std=(0.1405,))\n",
        "    ])\n",
        "transforms_aug_2 = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(p=1.),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.9583,), std=(0.1405,))\n",
        "    ])\n",
        "transforms_aug_3 = transforms.Compose([\n",
        "        transforms.RandomAffine(degrees=(-20, 20), translate=(0.1, 0.1), scale=(0.7, 1.4), fillcolor=(255,)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.9583,), std=(0.1405,))\n",
        "    ])\n",
        "transforms_aug_4 = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(size=(96, 96), scale=(0.5, 1.0), ratio=(3./4, 4./3)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.9583,), std=(0.1405,))\n",
        "    ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtUTezh289G9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Testset(Dataset):\n",
        "    def __init__(self, path, transforms, loader):\n",
        "        self.path = path\n",
        "        self.files = sorted(os.listdir(self.path)) # same order as csv\n",
        "        self.transforms = transforms\n",
        "        self.loader = loader\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        image_path = os.path.join(self.path, self.files[idx])\n",
        "        img = self.loader(image_path)\n",
        "        inputs = self.transforms(img)\n",
        "        return inputs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIAiMqrJaUGI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_set_aug_1 = torchvision.datasets.ImageFolder(train_path, transforms_aug_1, loader=myloader) # Data augmentation 1\n",
        "train_set_aug_2 = torchvision.datasets.ImageFolder(train_path, transforms_aug_2, loader=myloader) # Data augmentation 2\n",
        "train_set_aug_3 = torchvision.datasets.ImageFolder(train_path, transforms_aug_3, loader=myloader) # Data augmentation 3\n",
        "train_set_aug_4 = torchvision.datasets.ImageFolder(train_path, transforms_aug_4, loader=myloader) # Data augmentation 4\n",
        "train_set_nonaug = torchvision.datasets.ImageFolder(train_path, transforms_nonaug, loader=myloader) # w/o data augmentation\n",
        "test_set = Testset(test_path, transforms_nonaug, loader=myloader)\n",
        "classes = train_set_nonaug.classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "807KbNFpyhHG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Calculate mean and std\n",
        "# train_loader = DataLoader(train_set_nonaug, batch_size=1024, shuffle=False, pin_memory=True, num_workers=2)\n",
        "# x = torch.zeros((0, 1, 64, 64))\n",
        "# for batch_id, (inputs, labels) in enumerate(train_loader):\n",
        "#     x = torch.cat((x, inputs), dim=0)\n",
        "# print(x.mean(), x.std())\n",
        "# del x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrSoPfTryVkc",
        "colab_type": "text"
      },
      "source": [
        "# Data analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BRqQst90P41",
        "colab_type": "text"
      },
      "source": [
        "## Data augmentation example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zODfPledzhRN",
        "colab_type": "code",
        "outputId": "5edb2c22-2df7-4331-feec-2332280d67c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "# Data augmentation example\n",
        "i = 1063\n",
        "fig, axs = plt.subplots(1,5, figsize=(6, 6))\n",
        "axs[0].imshow(train_set_nonaug[i][0][0], cmap='gray')\n",
        "axs[0].axis('off')\n",
        "axs[1].imshow(train_set_aug_1[i][0][0], cmap='gray')\n",
        "axs[1].axis('off')\n",
        "axs[2].imshow(train_set_aug_2[i][0][0], cmap='gray')\n",
        "axs[2].axis('off')\n",
        "axs[3].imshow(train_set_aug_3[i][0][0], cmap='gray')\n",
        "axs[3].axis('off')\n",
        "axs[4].imshow(train_set_aug_4[i][0][0], cmap='gray')\n",
        "axs[4].axis('off')\n",
        "fig.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAABICAYAAABV5CYrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncIQYStfAwMDgCDGUroGBgcER8v8A9yympw/8SZUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x432 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufcTiFmn0T4w",
        "colab_type": "text"
      },
      "source": [
        "## Class distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HLywwnPys2U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = DataLoader(train_set_nonaug, batch_size=1024, shuffle=False, pin_memory=True, num_workers=2)\n",
        "label_col = torch.tensor([], dtype=torch.long)\n",
        "for batch_id, (inputs, labels) in enumerate(train_loader):\n",
        "    label_col = torch.cat((label_col, labels), dim=0)\n",
        "del inputs, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBzPzvGpz-OZ",
        "colab_type": "code",
        "outputId": "37138dc6-8f17-4b3f-9fa6-1df78365ee0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plt.title('Sample amount per category')\n",
        "plt.xlabel('Category')\n",
        "plt.ylabel('Sample number')\n",
        "plt.hist(label_col, bins=121)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWaH/D5XQbrm05swRAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EU30u4LImNY6",
        "colab_type": "text"
      },
      "source": [
        "# Define model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KPh25A0mQnt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    \"\"\"\n",
        "    ConvNet adapted from ResNet\n",
        "    \"\"\"\n",
        "    def __init__(self, precnn, num_classes=len(classes)):\n",
        "        super(CNN, self).__init__()\n",
        "        self.precnn = nn.Sequential(*list(precnn.children())[1:-1])\n",
        "        # Replace 1st conv to make it compatible with 1 channel input\n",
        "        self.conv0 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False) \n",
        "        self.fc = nn.Linear(512, num_classes, bias=True) # Replace the last fc, 512 for ResNet18, 2048 for ResNet50\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv0(x)\n",
        "        x = self.precnn(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        output = self.fc(x)\n",
        "        return output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcE9_URBdXOp",
        "colab_type": "text"
      },
      "source": [
        "# Training utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSwT2deWlrU3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0SMXYCTlsJb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(pred, targets):\n",
        "    num_correct = pred.argmax(dim=1).eq(targets).sum()\n",
        "    acc = num_correct.float() / targets.size(0)\n",
        "    return acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CvWnN4InfL4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def call_train(train_set, vali_set, cnn_save_path):\n",
        "    epochs = 1000\n",
        "    batch_size = 128\n",
        "    lr = 1e-3\n",
        "    patience = 5\n",
        "    best_loss = np.inf\n",
        "    best_epoch = 0\n",
        "    cnn = CNN(models.resnet18(pretrained=False))\n",
        "    cnn = cnn.cuda()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.AdamW(cnn.parameters(), lr=lr)\n",
        "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.3, patience=5, verbose=True)\n",
        "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=2)\n",
        "    vali_loader = DataLoader(vali_set, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=2)\n",
        "    train_losses = []\n",
        "    vali_losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        cnn.train()\n",
        "        train_loss = AverageMeter()\n",
        "        train_acc = AverageMeter()\n",
        "        vali_loss = AverageMeter()\n",
        "        vali_acc = AverageMeter()\n",
        "        for batch_id, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "            optimizer.zero_grad()\n",
        "            outputs = cnn(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            acc = accuracy(outputs, labels)\n",
        "            train_acc.update(acc, inputs.size(0))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss.update(loss.item(), inputs.size(0))\n",
        "            if batch_id % 100 == 0:\n",
        "                print(\"Batch: {}, loss {:.4f}.\".format(batch_id, loss.item()))\n",
        "        # Evaluation\n",
        "        cnn.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch_id, (inputs, labels) in enumerate(vali_loader):\n",
        "                inputs, labels = inputs.cuda(), labels.cuda()\n",
        "                outputs = cnn(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                acc = accuracy(outputs, labels)\n",
        "                vali_loss.update(loss.item(), inputs.size(0))\n",
        "                vali_acc.update(acc, inputs.size(0))\n",
        "            # Adjust learning rate\n",
        "            scheduler.step(train_loss.avg)\n",
        "        print(\"Epoch: {}/{}, training loss: {:.4f}, train acc: {:.4f}, vali loss: {:.4f}, vali acc: \\\n",
        "{:.4f}.\".format(epoch, epochs, train_loss.avg, train_acc.avg, vali_loss.avg, vali_acc.avg))\n",
        "        train_losses.append(train_loss.avg)\n",
        "        vali_losses.append(vali_loss.avg)\n",
        "        # Save best\n",
        "        if vali_loss.avg < best_loss:\n",
        "            best_loss = vali_loss.avg\n",
        "            best_epoch = epoch\n",
        "            torch.save(cnn.state_dict(), cnn_save_path)\n",
        "        # Early stopping\n",
        "        if epoch - best_epoch >= patience:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "\n",
        "    print(\"Best epoch: {}, best loss: {:.4f}.\".format(best_epoch, best_loss))\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77h9ZZvWljIN",
        "colab_type": "text"
      },
      "source": [
        "# KFold train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTaa1TwYlxTo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Collect all labels\n",
        "train_len = len(train_set_nonaug)\n",
        "all_id = np.array(range(train_len))\n",
        "label_col = np.array(train_set_nonaug.targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMPm7E8ElxW-",
        "colab_type": "code",
        "outputId": "dd4c86ae-0404-45cd-d56b-447251cfe5bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "skf = StratifiedKFold(n_splits=8, shuffle=True, random_state=42)\n",
        "i = 0\n",
        "cnn_save_paths = [os.path.join(save_path, \"res_ens_zhe_\" + str(i)) for i in range(8)]\n",
        "\n",
        "for train_id, vali_id in skf.split(all_id, label_col):\n",
        "    print(\"---Split {}---\".format(i))\n",
        "    train_set = ConcatDataset([Subset(train_set_nonaug, train_id), Subset(train_set_aug_1, train_id),\n",
        "                           Subset(train_set_aug_2, train_id), Subset(train_set_aug_3, train_id),\n",
        "                           Subset(train_set_aug_4, train_id)])\n",
        "    vali_set = Subset(train_set_nonaug, vali_id)\n",
        "    call_train(train_set, vali_set, cnn_save_paths[i])\n",
        "    i += 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---Split 0---\n",
            "Batch: 0, loss 4.7942.\n",
            "Batch: 100, loss 2.1579.\n",
            "Batch: 200, loss 2.0752.\n",
            "Batch: 300, loss 2.0971.\n",
            "Batch: 400, loss 1.8889.\n",
            "Batch: 500, loss 1.7546.\n",
            "Batch: 600, loss 1.3705.\n",
            "Batch: 700, loss 1.6816.\n",
            "Batch: 800, loss 1.4417.\n",
            "Batch: 900, loss 1.3370.\n",
            "Batch: 1000, loss 1.3004.\n",
            "Epoch: 0/1000, training loss: 1.6929, train acc: 0.5141, vali loss: 1.4904, vali acc: 0.5607.\n",
            "Batch: 0, loss 1.1664.\n",
            "Batch: 100, loss 1.2634.\n",
            "Batch: 200, loss 1.2249.\n",
            "Batch: 300, loss 1.1963.\n",
            "Batch: 400, loss 1.1652.\n",
            "Batch: 500, loss 1.2719.\n",
            "Batch: 600, loss 1.1345.\n",
            "Batch: 700, loss 0.9210.\n",
            "Batch: 800, loss 1.2823.\n",
            "Batch: 900, loss 1.1115.\n",
            "Batch: 1000, loss 1.3181.\n",
            "Epoch: 1/1000, training loss: 1.1049, train acc: 0.6535, vali loss: 1.0665, vali acc: 0.6762.\n",
            "Batch: 0, loss 1.0632.\n",
            "Batch: 100, loss 0.9529.\n",
            "Batch: 200, loss 0.8018.\n",
            "Batch: 300, loss 0.8414.\n",
            "Batch: 400, loss 0.8902.\n",
            "Batch: 500, loss 0.6979.\n",
            "Batch: 600, loss 1.0898.\n",
            "Batch: 700, loss 1.0457.\n",
            "Batch: 800, loss 0.7320.\n",
            "Batch: 900, loss 0.6361.\n",
            "Batch: 1000, loss 0.7719.\n",
            "Epoch: 2/1000, training loss: 0.9245, train acc: 0.7046, vali loss: 1.0057, vali acc: 0.6996.\n",
            "Batch: 0, loss 0.7720.\n",
            "Batch: 100, loss 0.7764.\n",
            "Batch: 200, loss 0.8206.\n",
            "Batch: 300, loss 0.8496.\n",
            "Batch: 400, loss 0.7940.\n",
            "Batch: 500, loss 0.7234.\n",
            "Batch: 600, loss 0.4506.\n",
            "Batch: 700, loss 0.7143.\n",
            "Batch: 800, loss 0.8074.\n",
            "Batch: 900, loss 0.9234.\n",
            "Batch: 1000, loss 0.9657.\n",
            "Epoch: 3/1000, training loss: 0.7995, train acc: 0.7363, vali loss: 1.0228, vali acc: 0.6912.\n",
            "Batch: 0, loss 0.7537.\n",
            "Batch: 100, loss 0.5817.\n",
            "Batch: 200, loss 0.6081.\n",
            "Batch: 300, loss 0.6478.\n",
            "Batch: 400, loss 0.4959.\n",
            "Batch: 500, loss 0.9454.\n",
            "Batch: 600, loss 0.7884.\n",
            "Batch: 700, loss 0.6979.\n",
            "Batch: 800, loss 0.5370.\n",
            "Batch: 900, loss 0.7390.\n",
            "Batch: 1000, loss 0.6694.\n",
            "Epoch: 4/1000, training loss: 0.7098, train acc: 0.7625, vali loss: 0.9590, vali acc: 0.7168.\n",
            "Batch: 0, loss 0.6217.\n",
            "Batch: 100, loss 0.6797.\n",
            "Batch: 200, loss 0.6231.\n",
            "Batch: 300, loss 0.6365.\n",
            "Batch: 400, loss 0.8504.\n",
            "Batch: 500, loss 0.6512.\n",
            "Batch: 600, loss 0.6209.\n",
            "Batch: 700, loss 0.6298.\n",
            "Batch: 800, loss 0.6339.\n",
            "Batch: 900, loss 0.7278.\n",
            "Batch: 1000, loss 0.5547.\n",
            "Epoch: 5/1000, training loss: 0.6328, train acc: 0.7858, vali loss: 0.9388, vali acc: 0.7199.\n",
            "Batch: 0, loss 0.5423.\n",
            "Batch: 100, loss 0.5469.\n",
            "Batch: 200, loss 0.6576.\n",
            "Batch: 300, loss 0.6295.\n",
            "Batch: 400, loss 0.6108.\n",
            "Batch: 500, loss 0.4972.\n",
            "Batch: 600, loss 0.4402.\n",
            "Batch: 700, loss 0.5301.\n",
            "Batch: 800, loss 0.5250.\n",
            "Batch: 900, loss 0.5033.\n",
            "Batch: 1000, loss 0.5534.\n",
            "Epoch: 6/1000, training loss: 0.5694, train acc: 0.8056, vali loss: 0.9787, vali acc: 0.7139.\n",
            "Batch: 0, loss 0.4501.\n",
            "Batch: 100, loss 0.3931.\n",
            "Batch: 200, loss 0.3827.\n",
            "Batch: 300, loss 0.6407.\n",
            "Batch: 400, loss 0.4746.\n",
            "Batch: 500, loss 0.5228.\n",
            "Batch: 600, loss 0.5048.\n",
            "Batch: 700, loss 0.5121.\n",
            "Batch: 800, loss 0.6647.\n",
            "Batch: 900, loss 0.5227.\n",
            "Batch: 1000, loss 0.4861.\n",
            "Epoch: 7/1000, training loss: 0.5082, train acc: 0.8251, vali loss: 1.0153, vali acc: 0.7165.\n",
            "Batch: 0, loss 0.5597.\n",
            "Batch: 100, loss 0.4218.\n",
            "Batch: 200, loss 0.5105.\n",
            "Batch: 300, loss 0.5043.\n",
            "Batch: 400, loss 0.4898.\n",
            "Batch: 500, loss 0.5250.\n",
            "Batch: 600, loss 0.3590.\n",
            "Batch: 700, loss 0.3910.\n",
            "Batch: 800, loss 0.3886.\n",
            "Batch: 900, loss 0.5427.\n",
            "Batch: 1000, loss 0.3904.\n",
            "Epoch: 8/1000, training loss: 0.4648, train acc: 0.8386, vali loss: 0.9770, vali acc: 0.7228.\n",
            "Batch: 0, loss 0.3092.\n",
            "Batch: 100, loss 0.3946.\n",
            "Batch: 200, loss 0.3036.\n",
            "Batch: 300, loss 0.5277.\n",
            "Batch: 400, loss 0.4234.\n",
            "Batch: 500, loss 0.4939.\n",
            "Batch: 600, loss 0.4325.\n",
            "Batch: 700, loss 0.3853.\n",
            "Batch: 800, loss 0.5599.\n",
            "Batch: 900, loss 0.2734.\n",
            "Batch: 1000, loss 0.3768.\n",
            "Epoch: 9/1000, training loss: 0.4215, train acc: 0.8528, vali loss: 1.0208, vali acc: 0.7315.\n",
            "Batch: 0, loss 0.4454.\n",
            "Batch: 100, loss 0.3415.\n",
            "Batch: 200, loss 0.4831.\n",
            "Batch: 300, loss 0.4397.\n",
            "Batch: 400, loss 0.6136.\n",
            "Batch: 500, loss 0.3900.\n",
            "Batch: 600, loss 0.3349.\n",
            "Batch: 700, loss 0.3407.\n",
            "Batch: 800, loss 0.4035.\n",
            "Batch: 900, loss 0.2312.\n",
            "Batch: 1000, loss 0.4934.\n",
            "Epoch: 10/1000, training loss: 0.3917, train acc: 0.8643, vali loss: 1.0705, vali acc: 0.7168.\n",
            "Early stopping\n",
            "Best epoch: 5, best loss: 0.9388.\n",
            "---Split 1---\n",
            "Batch: 0, loss 4.9972.\n",
            "Batch: 100, loss 2.1634.\n",
            "Batch: 200, loss 1.9552.\n",
            "Batch: 300, loss 1.6594.\n",
            "Batch: 400, loss 1.5963.\n",
            "Batch: 500, loss 1.5500.\n",
            "Batch: 600, loss 1.5151.\n",
            "Batch: 700, loss 1.4621.\n",
            "Batch: 800, loss 1.3190.\n",
            "Batch: 900, loss 1.1756.\n",
            "Batch: 1000, loss 1.3898.\n",
            "Epoch: 0/1000, training loss: 1.6943, train acc: 0.5139, vali loss: 1.3167, vali acc: 0.5994.\n",
            "Batch: 0, loss 1.2047.\n",
            "Batch: 100, loss 1.1945.\n",
            "Batch: 200, loss 1.0588.\n",
            "Batch: 300, loss 1.1489.\n",
            "Batch: 400, loss 1.1247.\n",
            "Batch: 500, loss 0.9472.\n",
            "Batch: 600, loss 0.9299.\n",
            "Batch: 700, loss 1.1952.\n",
            "Batch: 800, loss 0.9792.\n",
            "Batch: 900, loss 1.2026.\n",
            "Batch: 1000, loss 1.1179.\n",
            "Epoch: 1/1000, training loss: 1.1112, train acc: 0.6546, vali loss: 1.0489, vali acc: 0.6754.\n",
            "Batch: 0, loss 0.8534.\n",
            "Batch: 100, loss 0.8471.\n",
            "Batch: 200, loss 0.9290.\n",
            "Batch: 300, loss 0.9412.\n",
            "Batch: 400, loss 0.8801.\n",
            "Batch: 500, loss 0.9101.\n",
            "Batch: 600, loss 0.8824.\n",
            "Batch: 700, loss 0.9748.\n",
            "Batch: 800, loss 1.0316.\n",
            "Batch: 900, loss 0.8591.\n",
            "Batch: 1000, loss 0.8670.\n",
            "Epoch: 2/1000, training loss: 0.9203, train acc: 0.7040, vali loss: 0.9988, vali acc: 0.6930.\n",
            "Batch: 0, loss 0.9577.\n",
            "Batch: 100, loss 0.7288.\n",
            "Batch: 200, loss 0.7529.\n",
            "Batch: 300, loss 0.6805.\n",
            "Batch: 400, loss 0.8857.\n",
            "Batch: 500, loss 0.8815.\n",
            "Batch: 600, loss 0.8957.\n",
            "Batch: 700, loss 0.6607.\n",
            "Batch: 800, loss 0.7110.\n",
            "Batch: 900, loss 0.6633.\n",
            "Batch: 1000, loss 0.7650.\n",
            "Epoch: 3/1000, training loss: 0.8003, train acc: 0.7371, vali loss: 0.9251, vali acc: 0.7173.\n",
            "Batch: 0, loss 0.5539.\n",
            "Batch: 100, loss 0.7101.\n",
            "Batch: 200, loss 0.8907.\n",
            "Batch: 300, loss 0.5830.\n",
            "Batch: 400, loss 0.9545.\n",
            "Batch: 500, loss 0.6704.\n",
            "Batch: 600, loss 0.8911.\n",
            "Batch: 700, loss 0.7208.\n",
            "Batch: 800, loss 0.7572.\n",
            "Batch: 900, loss 0.6619.\n",
            "Batch: 1000, loss 0.5668.\n",
            "Epoch: 4/1000, training loss: 0.7071, train acc: 0.7639, vali loss: 1.0291, vali acc: 0.7002.\n",
            "Batch: 0, loss 0.7159.\n",
            "Batch: 100, loss 0.5818.\n",
            "Batch: 200, loss 0.5665.\n",
            "Batch: 300, loss 0.6563.\n",
            "Batch: 400, loss 0.4931.\n",
            "Batch: 500, loss 0.5476.\n",
            "Batch: 600, loss 0.6579.\n",
            "Batch: 700, loss 0.5434.\n",
            "Batch: 800, loss 0.6374.\n",
            "Batch: 900, loss 0.6391.\n",
            "Batch: 1000, loss 0.6655.\n",
            "Epoch: 5/1000, training loss: 0.6367, train acc: 0.7831, vali loss: 0.9833, vali acc: 0.7184.\n",
            "Batch: 0, loss 0.5674.\n",
            "Batch: 100, loss 0.5508.\n",
            "Batch: 200, loss 0.5746.\n",
            "Batch: 300, loss 0.4653.\n",
            "Batch: 400, loss 0.4711.\n",
            "Batch: 500, loss 0.5213.\n",
            "Batch: 600, loss 0.5330.\n",
            "Batch: 700, loss 0.6517.\n",
            "Batch: 800, loss 0.6455.\n",
            "Batch: 900, loss 0.4133.\n",
            "Batch: 1000, loss 0.5185.\n",
            "Epoch: 6/1000, training loss: 0.5679, train acc: 0.8050, vali loss: 0.9578, vali acc: 0.7281.\n",
            "Batch: 0, loss 0.4495.\n",
            "Batch: 100, loss 0.4684.\n",
            "Batch: 200, loss 0.4105.\n",
            "Batch: 300, loss 0.6199.\n",
            "Batch: 400, loss 0.6002.\n",
            "Batch: 500, loss 0.5561.\n",
            "Batch: 600, loss 0.5152.\n",
            "Batch: 700, loss 0.4573.\n",
            "Batch: 800, loss 0.4963.\n",
            "Batch: 900, loss 0.3874.\n",
            "Batch: 1000, loss 0.4370.\n",
            "Epoch: 7/1000, training loss: 0.5112, train acc: 0.8223, vali loss: 0.9427, vali acc: 0.7336.\n",
            "Batch: 0, loss 0.4589.\n",
            "Batch: 100, loss 0.4914.\n",
            "Batch: 200, loss 0.4578.\n",
            "Batch: 300, loss 0.5467.\n",
            "Batch: 400, loss 0.3048.\n",
            "Batch: 500, loss 0.3646.\n",
            "Batch: 600, loss 0.4388.\n",
            "Batch: 700, loss 0.4971.\n",
            "Batch: 800, loss 0.5908.\n",
            "Batch: 900, loss 0.3947.\n",
            "Batch: 1000, loss 0.6147.\n",
            "Epoch: 8/1000, training loss: 0.4649, train acc: 0.8393, vali loss: 0.9571, vali acc: 0.7410.\n",
            "Early stopping\n",
            "Best epoch: 3, best loss: 0.9251.\n",
            "---Split 2---\n",
            "Batch: 0, loss 4.8107.\n",
            "Batch: 100, loss 2.5351.\n",
            "Batch: 200, loss 1.8144.\n",
            "Batch: 300, loss 1.6748.\n",
            "Batch: 400, loss 1.5052.\n",
            "Batch: 500, loss 1.5268.\n",
            "Batch: 600, loss 1.2085.\n",
            "Batch: 700, loss 1.3286.\n",
            "Batch: 800, loss 1.3748.\n",
            "Batch: 900, loss 1.1019.\n",
            "Batch: 1000, loss 1.3108.\n",
            "Epoch: 0/1000, training loss: 1.6940, train acc: 0.5129, vali loss: 1.4811, vali acc: 0.5699.\n",
            "Batch: 0, loss 1.4196.\n",
            "Batch: 100, loss 1.2413.\n",
            "Batch: 200, loss 1.1492.\n",
            "Batch: 300, loss 1.2233.\n",
            "Batch: 400, loss 1.1803.\n",
            "Batch: 500, loss 1.2521.\n",
            "Batch: 600, loss 1.2768.\n",
            "Batch: 700, loss 1.0616.\n",
            "Batch: 800, loss 1.0265.\n",
            "Batch: 900, loss 0.8405.\n",
            "Batch: 1000, loss 1.0806.\n",
            "Epoch: 1/1000, training loss: 1.1180, train acc: 0.6510, vali loss: 1.1778, vali acc: 0.6508.\n",
            "Batch: 0, loss 0.8389.\n",
            "Batch: 100, loss 0.8537.\n",
            "Batch: 200, loss 0.9742.\n",
            "Batch: 300, loss 1.0249.\n",
            "Batch: 400, loss 0.9482.\n",
            "Batch: 500, loss 1.1488.\n",
            "Batch: 600, loss 0.8731.\n",
            "Batch: 700, loss 0.9550.\n",
            "Batch: 800, loss 1.0188.\n",
            "Batch: 900, loss 0.8355.\n",
            "Batch: 1000, loss 0.8761.\n",
            "Epoch: 2/1000, training loss: 0.9258, train acc: 0.7017, vali loss: 1.0158, vali acc: 0.6891.\n",
            "Batch: 0, loss 0.7458.\n",
            "Batch: 100, loss 0.7414.\n",
            "Batch: 200, loss 0.9031.\n",
            "Batch: 300, loss 0.9149.\n",
            "Batch: 400, loss 0.8335.\n",
            "Batch: 500, loss 1.0024.\n",
            "Batch: 600, loss 0.6980.\n",
            "Batch: 700, loss 0.7374.\n",
            "Batch: 800, loss 0.9058.\n",
            "Batch: 900, loss 0.7978.\n",
            "Batch: 1000, loss 0.8757.\n",
            "Epoch: 3/1000, training loss: 0.8058, train acc: 0.7345, vali loss: 0.9593, vali acc: 0.7144.\n",
            "Batch: 0, loss 0.5997.\n",
            "Batch: 100, loss 0.5798.\n",
            "Batch: 200, loss 0.7966.\n",
            "Batch: 300, loss 0.7418.\n",
            "Batch: 400, loss 0.9337.\n",
            "Batch: 500, loss 0.6181.\n",
            "Batch: 600, loss 0.6787.\n",
            "Batch: 700, loss 0.7712.\n",
            "Batch: 800, loss 0.5902.\n",
            "Batch: 900, loss 0.7185.\n",
            "Batch: 1000, loss 0.7796.\n",
            "Epoch: 4/1000, training loss: 0.7146, train acc: 0.7604, vali loss: 0.9444, vali acc: 0.7189.\n",
            "Batch: 0, loss 0.5085.\n",
            "Batch: 100, loss 0.8349.\n",
            "Batch: 200, loss 0.7324.\n",
            "Batch: 300, loss 0.6506.\n",
            "Batch: 400, loss 0.6322.\n",
            "Batch: 500, loss 0.8957.\n",
            "Batch: 600, loss 0.7369.\n",
            "Batch: 700, loss 0.5048.\n",
            "Batch: 800, loss 0.7040.\n",
            "Batch: 900, loss 0.6803.\n",
            "Batch: 1000, loss 0.6723.\n",
            "Epoch: 5/1000, training loss: 0.6406, train acc: 0.7822, vali loss: 0.9230, vali acc: 0.7286.\n",
            "Batch: 0, loss 0.6201.\n",
            "Batch: 100, loss 0.6072.\n",
            "Batch: 200, loss 0.5162.\n",
            "Batch: 300, loss 0.5378.\n",
            "Batch: 400, loss 0.7152.\n",
            "Batch: 500, loss 0.6677.\n",
            "Batch: 600, loss 0.4120.\n",
            "Batch: 700, loss 0.6350.\n",
            "Batch: 800, loss 0.4700.\n",
            "Batch: 900, loss 0.5830.\n",
            "Batch: 1000, loss 0.5226.\n",
            "Epoch: 6/1000, training loss: 0.5704, train acc: 0.8039, vali loss: 1.0337, vali acc: 0.7133.\n",
            "Batch: 0, loss 0.4386.\n",
            "Batch: 100, loss 0.6488.\n",
            "Batch: 200, loss 0.5742.\n",
            "Batch: 300, loss 0.5139.\n",
            "Batch: 400, loss 0.5642.\n",
            "Batch: 500, loss 0.6242.\n",
            "Batch: 600, loss 0.4830.\n",
            "Batch: 700, loss 0.5196.\n",
            "Batch: 800, loss 0.5587.\n",
            "Batch: 900, loss 0.4835.\n",
            "Batch: 1000, loss 0.4681.\n",
            "Epoch: 7/1000, training loss: 0.5166, train acc: 0.8211, vali loss: 1.0551, vali acc: 0.7120.\n",
            "Batch: 0, loss 0.6449.\n",
            "Batch: 100, loss 0.5755.\n",
            "Batch: 200, loss 0.5314.\n",
            "Batch: 300, loss 0.4046.\n",
            "Batch: 400, loss 0.4108.\n",
            "Batch: 500, loss 0.4116.\n",
            "Batch: 600, loss 0.4385.\n",
            "Batch: 700, loss 0.4463.\n",
            "Batch: 800, loss 0.5939.\n",
            "Batch: 900, loss 0.4202.\n",
            "Batch: 1000, loss 0.4086.\n",
            "Epoch: 8/1000, training loss: 0.4698, train acc: 0.8367, vali loss: 1.0662, vali acc: 0.7155.\n",
            "Batch: 0, loss 0.3571.\n",
            "Batch: 100, loss 0.3570.\n",
            "Batch: 200, loss 0.4573.\n",
            "Batch: 300, loss 0.4178.\n",
            "Batch: 400, loss 0.4415.\n",
            "Batch: 500, loss 0.3844.\n",
            "Batch: 600, loss 0.3906.\n",
            "Batch: 700, loss 0.3578.\n",
            "Batch: 800, loss 0.4918.\n",
            "Batch: 900, loss 0.4244.\n",
            "Batch: 1000, loss 0.5590.\n",
            "Epoch: 9/1000, training loss: 0.4274, train acc: 0.8517, vali loss: 0.9793, vali acc: 0.7389.\n",
            "Batch: 0, loss 0.4144.\n",
            "Batch: 100, loss 0.4188.\n",
            "Batch: 200, loss 0.3712.\n",
            "Batch: 300, loss 0.5343.\n",
            "Batch: 400, loss 0.5924.\n",
            "Batch: 500, loss 0.3785.\n",
            "Batch: 600, loss 0.4688.\n",
            "Batch: 700, loss 0.3681.\n",
            "Batch: 800, loss 0.3512.\n",
            "Batch: 900, loss 0.3756.\n",
            "Batch: 1000, loss 0.3483.\n",
            "Epoch: 10/1000, training loss: 0.3920, train acc: 0.8639, vali loss: 1.0956, vali acc: 0.7213.\n",
            "Early stopping\n",
            "Best epoch: 5, best loss: 0.9230.\n",
            "---Split 3---\n",
            "Batch: 0, loss 4.9296.\n",
            "Batch: 100, loss 2.1689.\n",
            "Batch: 200, loss 2.0351.\n",
            "Batch: 300, loss 1.8878.\n",
            "Batch: 400, loss 1.7627.\n",
            "Batch: 500, loss 1.4452.\n",
            "Batch: 600, loss 1.5610.\n",
            "Batch: 700, loss 1.3123.\n",
            "Batch: 800, loss 1.1406.\n",
            "Batch: 900, loss 1.1302.\n",
            "Batch: 1000, loss 1.1107.\n",
            "Epoch: 0/1000, training loss: 1.6955, train acc: 0.5160, vali loss: 1.3417, vali acc: 0.5955.\n",
            "Batch: 0, loss 1.3113.\n",
            "Batch: 100, loss 1.2680.\n",
            "Batch: 200, loss 1.0589.\n",
            "Batch: 300, loss 1.3265.\n",
            "Batch: 400, loss 1.1299.\n",
            "Batch: 500, loss 1.1090.\n",
            "Batch: 600, loss 1.0199.\n",
            "Batch: 700, loss 1.1190.\n",
            "Batch: 800, loss 1.1463.\n",
            "Batch: 900, loss 1.0395.\n",
            "Batch: 1000, loss 0.9466.\n",
            "Epoch: 1/1000, training loss: 1.1095, train acc: 0.6535, vali loss: 1.1090, vali acc: 0.6738.\n",
            "Batch: 0, loss 0.8783.\n",
            "Batch: 100, loss 1.1453.\n",
            "Batch: 200, loss 0.8443.\n",
            "Batch: 300, loss 0.7434.\n",
            "Batch: 400, loss 0.9796.\n",
            "Batch: 500, loss 0.8429.\n",
            "Batch: 600, loss 0.9542.\n",
            "Batch: 700, loss 1.0142.\n",
            "Batch: 800, loss 0.9122.\n",
            "Batch: 900, loss 0.9476.\n",
            "Batch: 1000, loss 0.9551.\n",
            "Epoch: 2/1000, training loss: 0.9220, train acc: 0.7025, vali loss: 0.9133, vali acc: 0.7165.\n",
            "Batch: 0, loss 0.6258.\n",
            "Batch: 100, loss 0.7823.\n",
            "Batch: 200, loss 0.8542.\n",
            "Batch: 300, loss 0.8959.\n",
            "Batch: 400, loss 0.8211.\n",
            "Batch: 500, loss 0.7124.\n",
            "Batch: 600, loss 0.7565.\n",
            "Batch: 700, loss 0.6799.\n",
            "Batch: 800, loss 0.7493.\n",
            "Batch: 900, loss 0.6886.\n",
            "Batch: 1000, loss 0.8594.\n",
            "Epoch: 3/1000, training loss: 0.8029, train acc: 0.7357, vali loss: 0.9174, vali acc: 0.7249.\n",
            "Batch: 0, loss 0.8779.\n",
            "Batch: 100, loss 0.7808.\n",
            "Batch: 200, loss 0.8997.\n",
            "Batch: 300, loss 0.6657.\n",
            "Batch: 400, loss 0.6259.\n",
            "Batch: 500, loss 0.7228.\n",
            "Batch: 600, loss 0.5913.\n",
            "Batch: 700, loss 0.8250.\n",
            "Batch: 800, loss 0.7636.\n",
            "Batch: 900, loss 0.9082.\n",
            "Batch: 1000, loss 0.7790.\n",
            "Epoch: 4/1000, training loss: 0.7081, train acc: 0.7620, vali loss: 1.0273, vali acc: 0.6907.\n",
            "Batch: 0, loss 0.6718.\n",
            "Batch: 100, loss 0.6292.\n",
            "Batch: 200, loss 0.7122.\n",
            "Batch: 300, loss 0.7238.\n",
            "Batch: 400, loss 0.6201.\n",
            "Batch: 500, loss 0.5879.\n",
            "Batch: 600, loss 0.4524.\n",
            "Batch: 700, loss 0.5544.\n",
            "Batch: 800, loss 0.5461.\n",
            "Batch: 900, loss 0.5136.\n",
            "Batch: 1000, loss 0.6027.\n",
            "Epoch: 5/1000, training loss: 0.6297, train acc: 0.7864, vali loss: 0.8553, vali acc: 0.7460.\n",
            "Batch: 0, loss 0.5450.\n",
            "Batch: 100, loss 0.5760.\n",
            "Batch: 200, loss 0.5264.\n",
            "Batch: 300, loss 0.4525.\n",
            "Batch: 400, loss 0.5135.\n",
            "Batch: 500, loss 0.5859.\n",
            "Batch: 600, loss 0.6178.\n",
            "Batch: 700, loss 0.5221.\n",
            "Batch: 800, loss 0.6870.\n",
            "Batch: 900, loss 0.6027.\n",
            "Batch: 1000, loss 0.6464.\n",
            "Epoch: 6/1000, training loss: 0.5651, train acc: 0.8055, vali loss: 0.9387, vali acc: 0.7210.\n",
            "Batch: 0, loss 0.5472.\n",
            "Batch: 100, loss 0.3826.\n",
            "Batch: 200, loss 0.4993.\n",
            "Batch: 300, loss 0.4216.\n",
            "Batch: 400, loss 0.5809.\n",
            "Batch: 500, loss 0.4902.\n",
            "Batch: 600, loss 0.6230.\n",
            "Batch: 700, loss 0.4535.\n",
            "Batch: 800, loss 0.3579.\n",
            "Batch: 900, loss 0.4239.\n",
            "Batch: 1000, loss 0.5007.\n",
            "Epoch: 7/1000, training loss: 0.5077, train acc: 0.8242, vali loss: 0.9110, vali acc: 0.7376.\n",
            "Batch: 0, loss 0.3663.\n",
            "Batch: 100, loss 0.4701.\n",
            "Batch: 200, loss 0.3945.\n",
            "Batch: 300, loss 0.4567.\n",
            "Batch: 400, loss 0.4300.\n",
            "Batch: 500, loss 0.6809.\n",
            "Batch: 600, loss 0.3444.\n",
            "Batch: 700, loss 0.6013.\n",
            "Batch: 800, loss 0.4569.\n",
            "Batch: 900, loss 0.4586.\n",
            "Batch: 1000, loss 0.3525.\n",
            "Epoch: 8/1000, training loss: 0.4637, train acc: 0.8402, vali loss: 0.9282, vali acc: 0.7326.\n",
            "Batch: 0, loss 0.3911.\n",
            "Batch: 100, loss 0.5576.\n",
            "Batch: 200, loss 0.3762.\n",
            "Batch: 300, loss 0.4257.\n",
            "Batch: 400, loss 0.3393.\n",
            "Batch: 500, loss 0.5042.\n",
            "Batch: 600, loss 0.3876.\n",
            "Batch: 700, loss 0.4103.\n",
            "Batch: 800, loss 0.3587.\n",
            "Batch: 900, loss 0.2713.\n",
            "Batch: 1000, loss 0.3535.\n",
            "Epoch: 9/1000, training loss: 0.4205, train acc: 0.8541, vali loss: 0.9515, vali acc: 0.7395.\n",
            "Batch: 0, loss 0.3203.\n",
            "Batch: 100, loss 0.5352.\n",
            "Batch: 200, loss 0.3302.\n",
            "Batch: 300, loss 0.3841.\n",
            "Batch: 400, loss 0.3677.\n",
            "Batch: 500, loss 0.4537.\n",
            "Batch: 600, loss 0.4449.\n",
            "Batch: 700, loss 0.3764.\n",
            "Batch: 800, loss 0.4924.\n",
            "Batch: 900, loss 0.4143.\n",
            "Batch: 1000, loss 0.2904.\n",
            "Epoch: 10/1000, training loss: 0.3876, train acc: 0.8651, vali loss: 0.9476, vali acc: 0.7368.\n",
            "Early stopping\n",
            "Best epoch: 5, best loss: 0.8553.\n",
            "---Split 4---\n",
            "Batch: 0, loss 4.9625.\n",
            "Batch: 100, loss 2.3793.\n",
            "Batch: 200, loss 2.0271.\n",
            "Batch: 300, loss 2.0295.\n",
            "Batch: 400, loss 1.4903.\n",
            "Batch: 500, loss 1.5155.\n",
            "Batch: 600, loss 1.5663.\n",
            "Batch: 700, loss 1.3680.\n",
            "Batch: 800, loss 1.3450.\n",
            "Batch: 900, loss 1.1761.\n",
            "Batch: 1000, loss 1.2176.\n",
            "Epoch: 0/1000, training loss: 1.6851, train acc: 0.5169, vali loss: 1.1774, vali acc: 0.6498.\n",
            "Batch: 0, loss 1.3460.\n",
            "Batch: 100, loss 1.2254.\n",
            "Batch: 200, loss 1.1258.\n",
            "Batch: 300, loss 0.9579.\n",
            "Batch: 400, loss 1.0669.\n",
            "Batch: 500, loss 1.1008.\n",
            "Batch: 600, loss 1.1526.\n",
            "Batch: 700, loss 0.9444.\n",
            "Batch: 800, loss 0.9813.\n",
            "Batch: 900, loss 1.0954.\n",
            "Batch: 1000, loss 1.0231.\n",
            "Epoch: 1/1000, training loss: 1.1076, train acc: 0.6545, vali loss: 1.1297, vali acc: 0.6627.\n",
            "Batch: 0, loss 0.9288.\n",
            "Batch: 100, loss 1.2918.\n",
            "Batch: 200, loss 0.8861.\n",
            "Batch: 300, loss 0.8087.\n",
            "Batch: 400, loss 0.9544.\n",
            "Batch: 500, loss 0.8812.\n",
            "Batch: 600, loss 0.8778.\n",
            "Batch: 700, loss 0.8817.\n",
            "Batch: 800, loss 1.0264.\n",
            "Batch: 900, loss 0.8706.\n",
            "Batch: 1000, loss 0.8209.\n",
            "Epoch: 2/1000, training loss: 0.9119, train acc: 0.7052, vali loss: 1.0298, vali acc: 0.6909.\n",
            "Batch: 0, loss 0.8002.\n",
            "Batch: 100, loss 0.7948.\n",
            "Batch: 200, loss 0.9311.\n",
            "Batch: 300, loss 0.7003.\n",
            "Batch: 400, loss 0.7596.\n",
            "Batch: 500, loss 0.8467.\n",
            "Batch: 600, loss 0.6628.\n",
            "Batch: 700, loss 0.8229.\n",
            "Batch: 800, loss 0.8042.\n",
            "Batch: 900, loss 0.7426.\n",
            "Batch: 1000, loss 0.6119.\n",
            "Epoch: 3/1000, training loss: 0.7949, train acc: 0.7373, vali loss: 1.0861, vali acc: 0.6817.\n",
            "Batch: 0, loss 0.7617.\n",
            "Batch: 100, loss 0.7029.\n",
            "Batch: 200, loss 0.8571.\n",
            "Batch: 300, loss 0.8404.\n",
            "Batch: 400, loss 0.6078.\n",
            "Batch: 500, loss 0.6370.\n",
            "Batch: 600, loss 0.5816.\n",
            "Batch: 700, loss 0.7470.\n",
            "Batch: 800, loss 0.6985.\n",
            "Batch: 900, loss 0.7642.\n",
            "Batch: 1000, loss 0.6931.\n",
            "Epoch: 4/1000, training loss: 0.7040, train acc: 0.7629, vali loss: 0.9230, vali acc: 0.7265.\n",
            "Batch: 0, loss 0.6160.\n",
            "Batch: 100, loss 0.7281.\n",
            "Batch: 200, loss 0.7565.\n",
            "Batch: 300, loss 0.5963.\n",
            "Batch: 400, loss 0.5580.\n",
            "Batch: 500, loss 0.5836.\n",
            "Batch: 600, loss 0.5572.\n",
            "Batch: 700, loss 0.5476.\n",
            "Batch: 800, loss 0.6527.\n",
            "Batch: 900, loss 0.6407.\n",
            "Batch: 1000, loss 0.4750.\n",
            "Epoch: 5/1000, training loss: 0.6274, train acc: 0.7856, vali loss: 0.9314, vali acc: 0.7141.\n",
            "Batch: 0, loss 0.5139.\n",
            "Batch: 100, loss 0.5685.\n",
            "Batch: 200, loss 0.4738.\n",
            "Batch: 300, loss 0.6670.\n",
            "Batch: 400, loss 0.6260.\n",
            "Batch: 500, loss 0.6112.\n",
            "Batch: 600, loss 0.6482.\n",
            "Batch: 700, loss 0.5927.\n",
            "Batch: 800, loss 0.4955.\n",
            "Batch: 900, loss 0.5419.\n",
            "Batch: 1000, loss 0.6320.\n",
            "Epoch: 6/1000, training loss: 0.5639, train acc: 0.8050, vali loss: 0.9276, vali acc: 0.7297.\n",
            "Batch: 0, loss 0.5668.\n",
            "Batch: 100, loss 0.3557.\n",
            "Batch: 200, loss 0.4145.\n",
            "Batch: 300, loss 0.5507.\n",
            "Batch: 400, loss 0.3663.\n",
            "Batch: 500, loss 0.4890.\n",
            "Batch: 600, loss 0.2410.\n",
            "Batch: 700, loss 0.5326.\n",
            "Batch: 800, loss 0.4160.\n",
            "Batch: 900, loss 0.3268.\n",
            "Batch: 1000, loss 0.5228.\n",
            "Epoch: 7/1000, training loss: 0.5076, train acc: 0.8244, vali loss: 0.9386, vali acc: 0.7355.\n",
            "Batch: 0, loss 0.4760.\n",
            "Batch: 100, loss 0.3441.\n",
            "Batch: 200, loss 0.5682.\n",
            "Batch: 300, loss 0.5121.\n",
            "Batch: 400, loss 0.3656.\n",
            "Batch: 500, loss 0.5854.\n",
            "Batch: 600, loss 0.3347.\n",
            "Batch: 700, loss 0.4912.\n",
            "Batch: 800, loss 0.5316.\n",
            "Batch: 900, loss 0.3779.\n",
            "Batch: 1000, loss 0.5158.\n",
            "Epoch: 8/1000, training loss: 0.4586, train acc: 0.8413, vali loss: 1.0732, vali acc: 0.7097.\n",
            "Batch: 0, loss 0.3758.\n",
            "Batch: 100, loss 0.3334.\n",
            "Batch: 200, loss 0.4487.\n",
            "Batch: 300, loss 0.3957.\n",
            "Batch: 400, loss 0.4618.\n",
            "Batch: 500, loss 0.3518.\n",
            "Batch: 600, loss 0.3679.\n",
            "Batch: 700, loss 0.4428.\n",
            "Batch: 800, loss 0.3182.\n",
            "Batch: 900, loss 0.4415.\n",
            "Batch: 1000, loss 0.4545.\n",
            "Epoch: 9/1000, training loss: 0.4180, train acc: 0.8540, vali loss: 0.9813, vali acc: 0.7466.\n",
            "Early stopping\n",
            "Best epoch: 4, best loss: 0.9230.\n",
            "---Split 5---\n",
            "Batch: 0, loss 4.9363.\n",
            "Batch: 100, loss 2.2217.\n",
            "Batch: 200, loss 1.9801.\n",
            "Batch: 300, loss 1.8133.\n",
            "Batch: 400, loss 1.5882.\n",
            "Batch: 500, loss 1.5408.\n",
            "Batch: 600, loss 1.4103.\n",
            "Batch: 700, loss 1.2284.\n",
            "Batch: 800, loss 1.4040.\n",
            "Batch: 900, loss 1.2785.\n",
            "Batch: 1000, loss 1.2835.\n",
            "Epoch: 0/1000, training loss: 1.6954, train acc: 0.5155, vali loss: 1.4421, vali acc: 0.5720.\n",
            "Batch: 0, loss 1.2501.\n",
            "Batch: 100, loss 1.2762.\n",
            "Batch: 200, loss 1.2759.\n",
            "Batch: 300, loss 1.3508.\n",
            "Batch: 400, loss 1.1628.\n",
            "Batch: 500, loss 1.3090.\n",
            "Batch: 600, loss 1.2809.\n",
            "Batch: 700, loss 0.9894.\n",
            "Batch: 800, loss 1.0477.\n",
            "Batch: 900, loss 1.2644.\n",
            "Batch: 1000, loss 1.0049.\n",
            "Epoch: 1/1000, training loss: 1.1245, train acc: 0.6494, vali loss: 1.0909, vali acc: 0.6624.\n",
            "Batch: 0, loss 0.9780.\n",
            "Batch: 100, loss 0.9986.\n",
            "Batch: 200, loss 0.9317.\n",
            "Batch: 300, loss 0.7858.\n",
            "Batch: 400, loss 0.8759.\n",
            "Batch: 500, loss 1.0025.\n",
            "Batch: 600, loss 0.7699.\n",
            "Batch: 700, loss 0.8526.\n",
            "Batch: 800, loss 0.8202.\n",
            "Batch: 900, loss 0.8991.\n",
            "Batch: 1000, loss 0.8281.\n",
            "Epoch: 2/1000, training loss: 0.9275, train acc: 0.7008, vali loss: 0.9667, vali acc: 0.7020.\n",
            "Batch: 0, loss 0.7297.\n",
            "Batch: 100, loss 0.7815.\n",
            "Batch: 200, loss 0.7745.\n",
            "Batch: 300, loss 0.9037.\n",
            "Batch: 400, loss 0.9142.\n",
            "Batch: 500, loss 0.9847.\n",
            "Batch: 600, loss 0.7670.\n",
            "Batch: 700, loss 0.8509.\n",
            "Batch: 800, loss 0.7579.\n",
            "Batch: 900, loss 0.9394.\n",
            "Batch: 1000, loss 0.7694.\n",
            "Epoch: 3/1000, training loss: 0.8038, train acc: 0.7359, vali loss: 0.9428, vali acc: 0.7099.\n",
            "Batch: 0, loss 0.6903.\n",
            "Batch: 100, loss 0.6222.\n",
            "Batch: 200, loss 0.9645.\n",
            "Batch: 300, loss 0.7816.\n",
            "Batch: 400, loss 0.6959.\n",
            "Batch: 500, loss 0.6431.\n",
            "Batch: 600, loss 0.6890.\n",
            "Batch: 700, loss 0.7157.\n",
            "Batch: 800, loss 0.9256.\n",
            "Batch: 900, loss 0.7543.\n",
            "Batch: 1000, loss 0.6546.\n",
            "Epoch: 4/1000, training loss: 0.7121, train acc: 0.7611, vali loss: 0.9124, vali acc: 0.7205.\n",
            "Batch: 0, loss 0.6426.\n",
            "Batch: 100, loss 0.6053.\n",
            "Batch: 200, loss 0.4944.\n",
            "Batch: 300, loss 0.6095.\n",
            "Batch: 400, loss 0.5036.\n",
            "Batch: 500, loss 0.7288.\n",
            "Batch: 600, loss 0.7065.\n",
            "Batch: 700, loss 0.4813.\n",
            "Batch: 800, loss 0.4579.\n",
            "Batch: 900, loss 0.6699.\n",
            "Batch: 1000, loss 0.5305.\n",
            "Epoch: 5/1000, training loss: 0.6375, train acc: 0.7840, vali loss: 1.0125, vali acc: 0.6986.\n",
            "Batch: 0, loss 0.5821.\n",
            "Batch: 100, loss 0.6450.\n",
            "Batch: 200, loss 0.4961.\n",
            "Batch: 300, loss 0.6201.\n",
            "Batch: 400, loss 0.5013.\n",
            "Batch: 500, loss 0.6941.\n",
            "Batch: 600, loss 0.5604.\n",
            "Batch: 700, loss 0.3934.\n",
            "Batch: 800, loss 0.5912.\n",
            "Batch: 900, loss 0.6473.\n",
            "Batch: 1000, loss 0.7125.\n",
            "Epoch: 6/1000, training loss: 0.5683, train acc: 0.8060, vali loss: 0.9074, vali acc: 0.7387.\n",
            "Batch: 0, loss 0.4973.\n",
            "Batch: 100, loss 0.4243.\n",
            "Batch: 200, loss 0.4624.\n",
            "Batch: 300, loss 0.3361.\n",
            "Batch: 400, loss 0.4531.\n",
            "Batch: 500, loss 0.3450.\n",
            "Batch: 600, loss 0.3937.\n",
            "Batch: 700, loss 0.5556.\n",
            "Batch: 800, loss 0.5030.\n",
            "Batch: 900, loss 0.5994.\n",
            "Batch: 1000, loss 0.4638.\n",
            "Epoch: 7/1000, training loss: 0.5138, train acc: 0.8231, vali loss: 1.0047, vali acc: 0.7089.\n",
            "Batch: 0, loss 0.4524.\n",
            "Batch: 100, loss 0.4440.\n",
            "Batch: 200, loss 0.5008.\n",
            "Batch: 300, loss 0.4528.\n",
            "Batch: 400, loss 0.5298.\n",
            "Batch: 500, loss 0.4607.\n",
            "Batch: 600, loss 0.5098.\n",
            "Batch: 700, loss 0.3766.\n",
            "Batch: 800, loss 0.5001.\n",
            "Batch: 900, loss 0.4081.\n",
            "Batch: 1000, loss 0.5586.\n",
            "Epoch: 8/1000, training loss: 0.4633, train acc: 0.8395, vali loss: 0.9285, vali acc: 0.7329.\n",
            "Batch: 0, loss 0.5663.\n",
            "Batch: 100, loss 0.3834.\n",
            "Batch: 200, loss 0.5112.\n",
            "Batch: 300, loss 0.3161.\n",
            "Batch: 400, loss 0.2270.\n",
            "Batch: 500, loss 0.3474.\n",
            "Batch: 600, loss 0.4165.\n",
            "Batch: 700, loss 0.3022.\n",
            "Batch: 800, loss 0.4574.\n",
            "Batch: 900, loss 0.5042.\n",
            "Batch: 1000, loss 0.5771.\n",
            "Epoch: 9/1000, training loss: 0.4247, train acc: 0.8524, vali loss: 0.9980, vali acc: 0.7247.\n",
            "Batch: 0, loss 0.4887.\n",
            "Batch: 100, loss 0.3437.\n",
            "Batch: 200, loss 0.3084.\n",
            "Batch: 300, loss 0.3166.\n",
            "Batch: 400, loss 0.2738.\n",
            "Batch: 500, loss 0.3468.\n",
            "Batch: 600, loss 0.4259.\n",
            "Batch: 700, loss 0.4254.\n",
            "Batch: 800, loss 0.6237.\n",
            "Batch: 900, loss 0.4624.\n",
            "Batch: 1000, loss 0.5065.\n",
            "Epoch: 10/1000, training loss: 0.3893, train acc: 0.8643, vali loss: 1.0058, vali acc: 0.7350.\n",
            "Batch: 0, loss 0.3078.\n",
            "Batch: 100, loss 0.2836.\n",
            "Batch: 200, loss 0.3620.\n",
            "Batch: 300, loss 0.3109.\n",
            "Batch: 400, loss 0.5499.\n",
            "Batch: 500, loss 0.4043.\n",
            "Batch: 600, loss 0.3126.\n",
            "Batch: 700, loss 0.3143.\n",
            "Batch: 800, loss 0.4121.\n",
            "Batch: 900, loss 0.3379.\n",
            "Batch: 1000, loss 0.3632.\n",
            "Epoch: 11/1000, training loss: 0.3574, train acc: 0.8744, vali loss: 1.1284, vali acc: 0.7260.\n",
            "Early stopping\n",
            "Best epoch: 6, best loss: 0.9074.\n",
            "---Split 6---\n",
            "Batch: 0, loss 5.0817.\n",
            "Batch: 100, loss 2.3012.\n",
            "Batch: 200, loss 1.8036.\n",
            "Batch: 300, loss 1.6946.\n",
            "Batch: 400, loss 1.4117.\n",
            "Batch: 500, loss 1.2804.\n",
            "Batch: 600, loss 1.3069.\n",
            "Batch: 700, loss 1.0912.\n",
            "Batch: 800, loss 1.2386.\n",
            "Batch: 900, loss 1.2975.\n",
            "Batch: 1000, loss 1.1953.\n",
            "Epoch: 0/1000, training loss: 1.6693, train acc: 0.5209, vali loss: 1.2857, vali acc: 0.6150.\n",
            "Batch: 0, loss 0.9852.\n",
            "Batch: 100, loss 0.9963.\n",
            "Batch: 200, loss 1.3260.\n",
            "Batch: 300, loss 1.0517.\n",
            "Batch: 400, loss 0.9952.\n",
            "Batch: 500, loss 1.0852.\n",
            "Batch: 600, loss 0.9929.\n",
            "Batch: 700, loss 0.9416.\n",
            "Batch: 800, loss 0.9619.\n",
            "Batch: 900, loss 0.9163.\n",
            "Batch: 1000, loss 0.9234.\n",
            "Epoch: 1/1000, training loss: 1.1055, train acc: 0.6553, vali loss: 1.2014, vali acc: 0.6564.\n",
            "Batch: 0, loss 0.9824.\n",
            "Batch: 100, loss 0.9979.\n",
            "Batch: 200, loss 0.7321.\n",
            "Batch: 300, loss 0.8494.\n",
            "Batch: 400, loss 1.2737.\n",
            "Batch: 500, loss 1.0883.\n",
            "Batch: 600, loss 0.8524.\n",
            "Batch: 700, loss 0.9435.\n",
            "Batch: 800, loss 0.7991.\n",
            "Batch: 900, loss 0.8740.\n",
            "Batch: 1000, loss 0.8855.\n",
            "Epoch: 2/1000, training loss: 0.9180, train acc: 0.7040, vali loss: 1.0080, vali acc: 0.7025.\n",
            "Batch: 0, loss 0.9001.\n",
            "Batch: 100, loss 0.9649.\n",
            "Batch: 200, loss 0.9199.\n",
            "Batch: 300, loss 0.8583.\n",
            "Batch: 400, loss 0.6270.\n",
            "Batch: 500, loss 0.6453.\n",
            "Batch: 600, loss 0.7765.\n",
            "Batch: 700, loss 0.7559.\n",
            "Batch: 800, loss 0.7611.\n",
            "Batch: 900, loss 0.8654.\n",
            "Batch: 1000, loss 0.7016.\n",
            "Epoch: 3/1000, training loss: 0.7949, train acc: 0.7380, vali loss: 0.9789, vali acc: 0.7081.\n",
            "Batch: 0, loss 0.7469.\n",
            "Batch: 100, loss 0.7990.\n",
            "Batch: 200, loss 0.6074.\n",
            "Batch: 300, loss 0.6706.\n",
            "Batch: 400, loss 0.7113.\n",
            "Batch: 500, loss 0.5954.\n",
            "Batch: 600, loss 0.6135.\n",
            "Batch: 700, loss 0.7049.\n",
            "Batch: 800, loss 0.7582.\n",
            "Batch: 900, loss 0.7788.\n",
            "Batch: 1000, loss 0.5608.\n",
            "Epoch: 4/1000, training loss: 0.7059, train acc: 0.7624, vali loss: 1.1450, vali acc: 0.6793.\n",
            "Batch: 0, loss 0.6582.\n",
            "Batch: 100, loss 0.6021.\n",
            "Batch: 200, loss 0.5911.\n",
            "Batch: 300, loss 0.5864.\n",
            "Batch: 400, loss 0.6255.\n",
            "Batch: 500, loss 0.5834.\n",
            "Batch: 600, loss 0.6022.\n",
            "Batch: 700, loss 0.5764.\n",
            "Batch: 800, loss 0.6452.\n",
            "Batch: 900, loss 0.7207.\n",
            "Batch: 1000, loss 0.5791.\n",
            "Epoch: 5/1000, training loss: 0.6260, train acc: 0.7859, vali loss: 0.9465, vali acc: 0.7336.\n",
            "Batch: 0, loss 0.5112.\n",
            "Batch: 100, loss 0.5071.\n",
            "Batch: 200, loss 0.4703.\n",
            "Batch: 300, loss 0.7388.\n",
            "Batch: 400, loss 0.5610.\n",
            "Batch: 500, loss 0.5211.\n",
            "Batch: 600, loss 0.5185.\n",
            "Batch: 700, loss 0.5268.\n",
            "Batch: 800, loss 0.5713.\n",
            "Batch: 900, loss 0.5437.\n",
            "Batch: 1000, loss 0.6134.\n",
            "Epoch: 6/1000, training loss: 0.5609, train acc: 0.8064, vali loss: 1.0024, vali acc: 0.7102.\n",
            "Batch: 0, loss 0.4529.\n",
            "Batch: 100, loss 0.3748.\n",
            "Batch: 200, loss 0.5299.\n",
            "Batch: 300, loss 0.5412.\n",
            "Batch: 400, loss 0.5917.\n",
            "Batch: 500, loss 0.5160.\n",
            "Batch: 600, loss 0.5262.\n",
            "Batch: 700, loss 0.5262.\n",
            "Batch: 800, loss 0.5933.\n",
            "Batch: 900, loss 0.5504.\n",
            "Batch: 1000, loss 0.4862.\n",
            "Epoch: 7/1000, training loss: 0.5056, train acc: 0.8249, vali loss: 1.2729, vali acc: 0.6593.\n",
            "Batch: 0, loss 0.4661.\n",
            "Batch: 100, loss 0.4172.\n",
            "Batch: 200, loss 0.4486.\n",
            "Batch: 300, loss 0.4898.\n",
            "Batch: 400, loss 0.3605.\n",
            "Batch: 500, loss 0.4263.\n",
            "Batch: 600, loss 0.4436.\n",
            "Batch: 700, loss 0.4367.\n",
            "Batch: 800, loss 0.5342.\n",
            "Batch: 900, loss 0.4500.\n",
            "Batch: 1000, loss 0.4313.\n",
            "Epoch: 8/1000, training loss: 0.4551, train acc: 0.8416, vali loss: 1.1628, vali acc: 0.7036.\n",
            "Batch: 0, loss 0.4336.\n",
            "Batch: 100, loss 0.3928.\n",
            "Batch: 200, loss 0.3544.\n",
            "Batch: 300, loss 0.3459.\n",
            "Batch: 400, loss 0.5091.\n",
            "Batch: 500, loss 0.4464.\n",
            "Batch: 600, loss 0.4695.\n",
            "Batch: 700, loss 0.4403.\n",
            "Batch: 800, loss 0.4241.\n",
            "Batch: 900, loss 0.4857.\n",
            "Batch: 1000, loss 0.4817.\n",
            "Epoch: 9/1000, training loss: 0.4174, train acc: 0.8547, vali loss: 1.0488, vali acc: 0.7263.\n",
            "Batch: 0, loss 0.4588.\n",
            "Batch: 100, loss 0.3581.\n",
            "Batch: 200, loss 0.3917.\n",
            "Batch: 300, loss 0.3619.\n",
            "Batch: 400, loss 0.3828.\n",
            "Batch: 500, loss 0.4054.\n",
            "Batch: 600, loss 0.3902.\n",
            "Batch: 700, loss 0.4159.\n",
            "Batch: 800, loss 0.3406.\n",
            "Batch: 900, loss 0.4111.\n",
            "Batch: 1000, loss 0.3765.\n",
            "Epoch: 10/1000, training loss: 0.3850, train acc: 0.8667, vali loss: 1.1625, vali acc: 0.7147.\n",
            "Early stopping\n",
            "Best epoch: 5, best loss: 0.9465.\n",
            "---Split 7---\n",
            "Batch: 0, loss 4.9648.\n",
            "Batch: 100, loss 2.3433.\n",
            "Batch: 200, loss 1.8163.\n",
            "Batch: 300, loss 1.8255.\n",
            "Batch: 400, loss 1.6412.\n",
            "Batch: 500, loss 1.9685.\n",
            "Batch: 600, loss 1.5285.\n",
            "Batch: 700, loss 1.4067.\n",
            "Batch: 800, loss 1.3058.\n",
            "Batch: 900, loss 1.3415.\n",
            "Batch: 1000, loss 1.3724.\n",
            "Epoch: 0/1000, training loss: 1.7018, train acc: 0.5115, vali loss: 1.2544, vali acc: 0.6229.\n",
            "Batch: 0, loss 1.2317.\n",
            "Batch: 100, loss 1.3947.\n",
            "Batch: 200, loss 1.3188.\n",
            "Batch: 300, loss 0.9612.\n",
            "Batch: 400, loss 1.1857.\n",
            "Batch: 500, loss 1.4362.\n",
            "Batch: 600, loss 1.1727.\n",
            "Batch: 700, loss 1.1322.\n",
            "Batch: 800, loss 1.0859.\n",
            "Batch: 900, loss 1.2108.\n",
            "Batch: 1000, loss 0.9894.\n",
            "Epoch: 1/1000, training loss: 1.1219, train acc: 0.6501, vali loss: 1.1643, vali acc: 0.6514.\n",
            "Batch: 0, loss 0.9623.\n",
            "Batch: 100, loss 1.0652.\n",
            "Batch: 200, loss 0.8561.\n",
            "Batch: 300, loss 0.9022.\n",
            "Batch: 400, loss 1.0496.\n",
            "Batch: 500, loss 0.8640.\n",
            "Batch: 600, loss 0.9893.\n",
            "Batch: 700, loss 0.9607.\n",
            "Batch: 800, loss 0.9707.\n",
            "Batch: 900, loss 0.9528.\n",
            "Batch: 1000, loss 0.7803.\n",
            "Epoch: 2/1000, training loss: 0.9294, train acc: 0.7014, vali loss: 0.9558, vali acc: 0.7041.\n",
            "Batch: 0, loss 0.8878.\n",
            "Batch: 100, loss 0.8748.\n",
            "Batch: 200, loss 0.7386.\n",
            "Batch: 300, loss 0.9483.\n",
            "Batch: 400, loss 0.5283.\n",
            "Batch: 500, loss 0.7547.\n",
            "Batch: 600, loss 0.7553.\n",
            "Batch: 700, loss 0.8240.\n",
            "Batch: 800, loss 0.8412.\n",
            "Batch: 900, loss 0.5797.\n",
            "Batch: 1000, loss 0.7944.\n",
            "Epoch: 3/1000, training loss: 0.8042, train acc: 0.7358, vali loss: 1.0807, vali acc: 0.6817.\n",
            "Batch: 0, loss 0.7513.\n",
            "Batch: 100, loss 0.6982.\n",
            "Batch: 200, loss 0.6906.\n",
            "Batch: 300, loss 0.6131.\n",
            "Batch: 400, loss 0.9076.\n",
            "Batch: 500, loss 0.7161.\n",
            "Batch: 600, loss 0.5140.\n",
            "Batch: 700, loss 0.6632.\n",
            "Batch: 800, loss 0.6000.\n",
            "Batch: 900, loss 0.7793.\n",
            "Batch: 1000, loss 0.6818.\n",
            "Epoch: 4/1000, training loss: 0.7152, train acc: 0.7605, vali loss: 0.8985, vali acc: 0.7223.\n",
            "Batch: 0, loss 0.6755.\n",
            "Batch: 100, loss 0.6508.\n",
            "Batch: 200, loss 0.6944.\n",
            "Batch: 300, loss 0.7752.\n",
            "Batch: 400, loss 0.8118.\n",
            "Batch: 500, loss 0.8538.\n",
            "Batch: 600, loss 0.6805.\n",
            "Batch: 700, loss 0.6323.\n",
            "Batch: 800, loss 0.6180.\n",
            "Batch: 900, loss 0.7258.\n",
            "Batch: 1000, loss 0.6835.\n",
            "Epoch: 5/1000, training loss: 0.6354, train acc: 0.7838, vali loss: 0.9119, vali acc: 0.7223.\n",
            "Batch: 0, loss 0.4821.\n",
            "Batch: 100, loss 0.8015.\n",
            "Batch: 200, loss 0.5016.\n",
            "Batch: 300, loss 0.5291.\n",
            "Batch: 400, loss 0.5537.\n",
            "Batch: 500, loss 0.5901.\n",
            "Batch: 600, loss 0.5494.\n",
            "Batch: 700, loss 0.5940.\n",
            "Batch: 800, loss 0.5701.\n",
            "Batch: 900, loss 0.6150.\n",
            "Batch: 1000, loss 0.6440.\n",
            "Epoch: 6/1000, training loss: 0.5718, train acc: 0.8037, vali loss: 0.9292, vali acc: 0.7226.\n",
            "Batch: 0, loss 0.7152.\n",
            "Batch: 100, loss 0.5523.\n",
            "Batch: 200, loss 0.5304.\n",
            "Batch: 300, loss 0.5517.\n",
            "Batch: 400, loss 0.5653.\n",
            "Batch: 500, loss 0.6216.\n",
            "Batch: 600, loss 0.4801.\n",
            "Batch: 700, loss 0.4433.\n",
            "Batch: 800, loss 0.5345.\n",
            "Batch: 900, loss 0.5016.\n",
            "Batch: 1000, loss 0.3868.\n",
            "Epoch: 7/1000, training loss: 0.5163, train acc: 0.8215, vali loss: 0.9109, vali acc: 0.7431.\n",
            "Batch: 0, loss 0.4139.\n",
            "Batch: 100, loss 0.3615.\n",
            "Batch: 200, loss 0.6183.\n",
            "Batch: 300, loss 0.4074.\n",
            "Batch: 400, loss 0.4714.\n",
            "Batch: 500, loss 0.4651.\n",
            "Batch: 600, loss 0.3763.\n",
            "Batch: 700, loss 0.6396.\n",
            "Batch: 800, loss 0.4624.\n",
            "Batch: 900, loss 0.4676.\n",
            "Batch: 1000, loss 0.4941.\n",
            "Epoch: 8/1000, training loss: 0.4664, train acc: 0.8386, vali loss: 0.9539, vali acc: 0.7331.\n",
            "Batch: 0, loss 0.3815.\n",
            "Batch: 100, loss 0.4225.\n",
            "Batch: 200, loss 0.4526.\n",
            "Batch: 300, loss 0.5336.\n",
            "Batch: 400, loss 0.5711.\n",
            "Batch: 500, loss 0.5022.\n",
            "Batch: 600, loss 0.4457.\n",
            "Batch: 700, loss 0.4764.\n",
            "Batch: 800, loss 0.3011.\n",
            "Batch: 900, loss 0.4915.\n",
            "Batch: 1000, loss 0.4234.\n",
            "Epoch: 9/1000, training loss: 0.4271, train acc: 0.8512, vali loss: 0.9657, vali acc: 0.7360.\n",
            "Early stopping\n",
            "Best epoch: 4, best loss: 0.8985.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uw9dVQGiowOs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THMi6x1Dxw6R",
        "colab_type": "code",
        "outputId": "38501df6-b550-4601-9818-bf9b736e4834",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Apr  3 17:32:31 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   77C    P0    31W /  70W |   1263MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gf74ZmIjdAeQ",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFsHsO0DdKzY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Collect all labels\n",
        "train_len = len(train_set_nonaug)\n",
        "all_id = np.array(range(train_len))\n",
        "label_col = np.array(train_set_nonaug.targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRDLLob3dTP4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Stratifiedly split 1/8 as vali set\n",
        "train_id, vali_id, train_label, vali_label = train_test_split(all_id, label_col, test_size=1/8, random_state=42, stratify=label_col)\n",
        "vali_set = Subset(train_set_nonaug, vali_id)\n",
        "train_set = ConcatDataset([Subset(train_set_nonaug, train_id), Subset(train_set_aug_1, train_id),\n",
        "                           Subset(train_set_aug_2, train_id), Subset(train_set_aug_3, train_id),\n",
        "                           Subset(train_set_aug_4, train_id)])\n",
        "cnn_save_path = os.path.join(save_path, \"res_one_zhe\")\n",
        "# Train\n",
        "call_train(train_set, vali_set, cnn_save_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_J4my09w0Dw",
        "colab_type": "text"
      },
      "source": [
        "# KFold Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1o2HKx3w3Go",
        "colab_type": "code",
        "outputId": "db1a0a69-e8fb-4768-ee5b-6751a956f2db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "source": [
        "test_loader = DataLoader(test_set, batch_size=1024, shuffle=False, pin_memory=True, num_workers=2)\n",
        "cnn_save_paths = [os.path.join(save_path, \"res_ens_zhe_\" + str(i)) for i in range(8)]\n",
        "cnn_pred = CNN(models.resnet18(pretrained=False))\n",
        "cnn_pred = cnn_pred.cuda()\n",
        "out_collection = []\n",
        "\n",
        "for i in range(8):\n",
        "    print(\"---Split {}---\".format(i))\n",
        "    cnn_pred.load_state_dict(torch.load(cnn_save_paths[i]))\n",
        "    cnn_pred.eval()\n",
        "    out_collection_i = np.zeros((0, len(classes)))\n",
        "    with torch.no_grad():\n",
        "        for batch_id, inputs in enumerate(test_loader):\n",
        "            if batch_id % 50 == 0:\n",
        "                print(\"Batch: {}.\".format(batch_id))\n",
        "            inputs = inputs.cuda()\n",
        "            outputs = cnn_pred(inputs)\n",
        "            outputs = F.softmax(outputs, dim=1)\n",
        "            out_collection_i = np.concatenate((out_collection_i, outputs.cpu().numpy()), axis=0)\n",
        "    out_collection.append(out_collection_i)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---Split 0---\n",
            "Batch: 0.\n",
            "Batch: 50.\n",
            "Batch: 100.\n",
            "---Split 1---\n",
            "Batch: 0.\n",
            "Batch: 50.\n",
            "Batch: 100.\n",
            "---Split 2---\n",
            "Batch: 0.\n",
            "Batch: 50.\n",
            "Batch: 100.\n",
            "---Split 3---\n",
            "Batch: 0.\n",
            "Batch: 50.\n",
            "Batch: 100.\n",
            "---Split 4---\n",
            "Batch: 0.\n",
            "Batch: 50.\n",
            "Batch: 100.\n",
            "---Split 5---\n",
            "Batch: 0.\n",
            "Batch: 50.\n",
            "Batch: 100.\n",
            "---Split 6---\n",
            "Batch: 0.\n",
            "Batch: 50.\n",
            "Batch: 100.\n",
            "---Split 7---\n",
            "Batch: 0.\n",
            "Batch: 50.\n",
            "Batch: 100.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vKnCtwf1OJX",
        "colab_type": "code",
        "outputId": "7c9b710f-9485-4765-8bdf-995f3b6c831d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "# Averaging prediction\n",
        "out_collection = np.array(out_collection)\n",
        "out_collection = out_collection.mean(axis=0)\n",
        "fig, axs = plt.subplots(2)\n",
        "axs[0].hist(out_collection.argmax(axis=1), bins=121)\n",
        "axs[1].hist(label_col, bins=121)\n",
        "fig.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmVlGHPpmZhlx6JuZZeT/AZu8ujAMkP/IAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWWpcmPBEDty",
        "colab_type": "text"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzZQ4ipsajg-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loader = DataLoader(test_set, batch_size=1024, shuffle=False, pin_memory=True, num_workers=2)\n",
        "cnn_pred =  CNN(models.resnet18(pretrained=False))\n",
        "cnn_pred = cnn_pred.cuda()\n",
        "cnn_pred.load_state_dict(torch.load(cnn_save_path))\n",
        "cnn_pred.eval()\n",
        "out_collection = np.zeros((0, len(classes)))\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_id, inputs in enumerate(test_loader):\n",
        "        if batch_id % 50 == 0:\n",
        "            print(\"Batch: {}.\".format(batch_id))\n",
        "        inputs = inputs.cuda()\n",
        "        outputs = cnn_pred(inputs)\n",
        "        outputs = F.softmax(outputs, dim=1)\n",
        "        out_collection = np.concatenate((out_collection, outputs.cpu().numpy()), axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCrb6zmEMVaj",
        "colab_type": "code",
        "outputId": "202fd82a-10b7-4a1c-d17d-003509a554e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "fig, axs = plt.subplots(2)\n",
        "axs[0].hist(out_collection.argmax(axis=1), bins=121)\n",
        "axs[1].hist(label_col, bins=121)\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wdlxMSjrOU/WAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1caLA470h5fk",
        "colab_type": "text"
      },
      "source": [
        "# Submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWNcffR5zL-j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission_csv = pd.read_csv(os.path.join(csv_path, \"sampleSubmission.csv\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88kmYOMLwZ0s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Solve class misalignment\n",
        "indices = []\n",
        "for i in submission_csv.columns.to_list()[1:]:\n",
        "    indices.append(classes.index(i))\n",
        "classes_sorted = (np.array(classes))[indices]\n",
        "out_collection = out_collection[:, indices]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDtwgXx5F0vp",
        "colab_type": "code",
        "outputId": "edb018c7-63b3-402b-deda-857ec7188360",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        }
      },
      "source": [
        "submission_csv.loc[:, submission_csv.columns != 'image'] = out_collection\n",
        "submission_csv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image</th>\n",
              "      <th>acantharia_protist_big_center</th>\n",
              "      <th>acantharia_protist_halo</th>\n",
              "      <th>acantharia_protist</th>\n",
              "      <th>amphipods</th>\n",
              "      <th>appendicularian_fritillaridae</th>\n",
              "      <th>appendicularian_s_shape</th>\n",
              "      <th>appendicularian_slight_curve</th>\n",
              "      <th>appendicularian_straight</th>\n",
              "      <th>artifacts_edge</th>\n",
              "      <th>artifacts</th>\n",
              "      <th>chaetognath_non_sagitta</th>\n",
              "      <th>chaetognath_other</th>\n",
              "      <th>chaetognath_sagitta</th>\n",
              "      <th>chordate_type1</th>\n",
              "      <th>copepod_calanoid_eggs</th>\n",
              "      <th>copepod_calanoid_eucalanus</th>\n",
              "      <th>copepod_calanoid_flatheads</th>\n",
              "      <th>copepod_calanoid_frillyAntennae</th>\n",
              "      <th>copepod_calanoid_large_side_antennatucked</th>\n",
              "      <th>copepod_calanoid_large</th>\n",
              "      <th>copepod_calanoid_octomoms</th>\n",
              "      <th>copepod_calanoid_small_longantennae</th>\n",
              "      <th>copepod_calanoid</th>\n",
              "      <th>copepod_cyclopoid_copilia</th>\n",
              "      <th>copepod_cyclopoid_oithona_eggs</th>\n",
              "      <th>copepod_cyclopoid_oithona</th>\n",
              "      <th>copepod_other</th>\n",
              "      <th>crustacean_other</th>\n",
              "      <th>ctenophore_cestid</th>\n",
              "      <th>ctenophore_cydippid_no_tentacles</th>\n",
              "      <th>ctenophore_cydippid_tentacles</th>\n",
              "      <th>ctenophore_lobate</th>\n",
              "      <th>decapods</th>\n",
              "      <th>detritus_blob</th>\n",
              "      <th>detritus_filamentous</th>\n",
              "      <th>detritus_other</th>\n",
              "      <th>diatom_chain_string</th>\n",
              "      <th>diatom_chain_tube</th>\n",
              "      <th>echinoderm_larva_pluteus_brittlestar</th>\n",
              "      <th>...</th>\n",
              "      <th>polychaete</th>\n",
              "      <th>protist_dark_center</th>\n",
              "      <th>protist_fuzzy_olive</th>\n",
              "      <th>protist_noctiluca</th>\n",
              "      <th>protist_other</th>\n",
              "      <th>protist_star</th>\n",
              "      <th>pteropod_butterfly</th>\n",
              "      <th>pteropod_theco_dev_seq</th>\n",
              "      <th>pteropod_triangle</th>\n",
              "      <th>radiolarian_chain</th>\n",
              "      <th>radiolarian_colony</th>\n",
              "      <th>shrimp_caridean</th>\n",
              "      <th>shrimp_sergestidae</th>\n",
              "      <th>shrimp_zoea</th>\n",
              "      <th>shrimp-like_other</th>\n",
              "      <th>siphonophore_calycophoran_abylidae</th>\n",
              "      <th>siphonophore_calycophoran_rocketship_adult</th>\n",
              "      <th>siphonophore_calycophoran_rocketship_young</th>\n",
              "      <th>siphonophore_calycophoran_sphaeronectes_stem</th>\n",
              "      <th>siphonophore_calycophoran_sphaeronectes_young</th>\n",
              "      <th>siphonophore_calycophoran_sphaeronectes</th>\n",
              "      <th>siphonophore_other_parts</th>\n",
              "      <th>siphonophore_partial</th>\n",
              "      <th>siphonophore_physonect_young</th>\n",
              "      <th>siphonophore_physonect</th>\n",
              "      <th>stomatopod</th>\n",
              "      <th>tornaria_acorn_worm_larvae</th>\n",
              "      <th>trichodesmium_bowtie</th>\n",
              "      <th>trichodesmium_multiple</th>\n",
              "      <th>trichodesmium_puff</th>\n",
              "      <th>trichodesmium_tuft</th>\n",
              "      <th>trochophore_larvae</th>\n",
              "      <th>tunicate_doliolid_nurse</th>\n",
              "      <th>tunicate_doliolid</th>\n",
              "      <th>tunicate_partial</th>\n",
              "      <th>tunicate_salp_chains</th>\n",
              "      <th>tunicate_salp</th>\n",
              "      <th>unknown_blobs_and_smudges</th>\n",
              "      <th>unknown_sticks</th>\n",
              "      <th>unknown_unclassified</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.jpg</td>\n",
              "      <td>2.901500e-09</td>\n",
              "      <td>9.580143e-07</td>\n",
              "      <td>1.411901e-03</td>\n",
              "      <td>6.994398e-06</td>\n",
              "      <td>1.794654e-06</td>\n",
              "      <td>5.914893e-04</td>\n",
              "      <td>4.096868e-04</td>\n",
              "      <td>8.326411e-04</td>\n",
              "      <td>2.511762e-03</td>\n",
              "      <td>1.256936e-02</td>\n",
              "      <td>2.402563e-05</td>\n",
              "      <td>4.317521e-03</td>\n",
              "      <td>4.721258e-06</td>\n",
              "      <td>4.019559e-06</td>\n",
              "      <td>6.244769e-06</td>\n",
              "      <td>7.562546e-07</td>\n",
              "      <td>7.905654e-06</td>\n",
              "      <td>7.813334e-06</td>\n",
              "      <td>4.353316e-06</td>\n",
              "      <td>2.570538e-07</td>\n",
              "      <td>5.830547e-08</td>\n",
              "      <td>8.540587e-07</td>\n",
              "      <td>4.700779e-05</td>\n",
              "      <td>6.929275e-05</td>\n",
              "      <td>2.046752e-04</td>\n",
              "      <td>7.695224e-04</td>\n",
              "      <td>7.281881e-06</td>\n",
              "      <td>3.206357e-04</td>\n",
              "      <td>6.611596e-07</td>\n",
              "      <td>7.064684e-05</td>\n",
              "      <td>7.489184e-06</td>\n",
              "      <td>1.004838e-08</td>\n",
              "      <td>5.234111e-06</td>\n",
              "      <td>1.429736e-02</td>\n",
              "      <td>1.897734e-02</td>\n",
              "      <td>9.977256e-02</td>\n",
              "      <td>5.051068e-02</td>\n",
              "      <td>2.488683e-01</td>\n",
              "      <td>4.933850e-07</td>\n",
              "      <td>...</td>\n",
              "      <td>2.266984e-04</td>\n",
              "      <td>3.969057e-04</td>\n",
              "      <td>1.775378e-05</td>\n",
              "      <td>1.807486e-03</td>\n",
              "      <td>1.704964e-02</td>\n",
              "      <td>2.191805e-04</td>\n",
              "      <td>7.849830e-06</td>\n",
              "      <td>1.458904e-10</td>\n",
              "      <td>2.065245e-06</td>\n",
              "      <td>1.815274e-02</td>\n",
              "      <td>1.423981e-03</td>\n",
              "      <td>7.268586e-09</td>\n",
              "      <td>5.212136e-08</td>\n",
              "      <td>3.743716e-06</td>\n",
              "      <td>8.130995e-06</td>\n",
              "      <td>8.291844e-07</td>\n",
              "      <td>6.497686e-07</td>\n",
              "      <td>6.754752e-06</td>\n",
              "      <td>6.406052e-08</td>\n",
              "      <td>2.398454e-05</td>\n",
              "      <td>2.745449e-06</td>\n",
              "      <td>1.191664e-06</td>\n",
              "      <td>2.858809e-08</td>\n",
              "      <td>5.830425e-08</td>\n",
              "      <td>8.620532e-06</td>\n",
              "      <td>1.624100e-08</td>\n",
              "      <td>3.005204e-05</td>\n",
              "      <td>5.328665e-05</td>\n",
              "      <td>9.034289e-07</td>\n",
              "      <td>9.290713e-05</td>\n",
              "      <td>5.919811e-04</td>\n",
              "      <td>8.664326e-07</td>\n",
              "      <td>9.750986e-05</td>\n",
              "      <td>3.136205e-03</td>\n",
              "      <td>2.469939e-05</td>\n",
              "      <td>7.529788e-05</td>\n",
              "      <td>6.550036e-05</td>\n",
              "      <td>3.293713e-02</td>\n",
              "      <td>4.324879e-02</td>\n",
              "      <td>0.001341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10.jpg</td>\n",
              "      <td>2.947897e-10</td>\n",
              "      <td>7.255509e-07</td>\n",
              "      <td>1.540880e-05</td>\n",
              "      <td>8.632581e-05</td>\n",
              "      <td>8.762593e-09</td>\n",
              "      <td>3.088071e-08</td>\n",
              "      <td>6.287839e-08</td>\n",
              "      <td>2.482221e-07</td>\n",
              "      <td>1.992036e-09</td>\n",
              "      <td>6.036812e-10</td>\n",
              "      <td>1.183950e-07</td>\n",
              "      <td>2.166513e-05</td>\n",
              "      <td>2.613370e-07</td>\n",
              "      <td>5.335198e-13</td>\n",
              "      <td>5.376071e-05</td>\n",
              "      <td>3.531697e-05</td>\n",
              "      <td>5.896602e-03</td>\n",
              "      <td>1.980012e-04</td>\n",
              "      <td>1.095512e-05</td>\n",
              "      <td>3.047975e-06</td>\n",
              "      <td>3.508395e-05</td>\n",
              "      <td>9.665737e-07</td>\n",
              "      <td>6.718383e-03</td>\n",
              "      <td>6.595980e-10</td>\n",
              "      <td>2.080032e-01</td>\n",
              "      <td>7.663618e-01</td>\n",
              "      <td>3.046630e-05</td>\n",
              "      <td>7.849906e-05</td>\n",
              "      <td>9.453302e-13</td>\n",
              "      <td>5.463635e-10</td>\n",
              "      <td>9.463167e-09</td>\n",
              "      <td>4.843815e-11</td>\n",
              "      <td>6.046974e-08</td>\n",
              "      <td>5.218871e-05</td>\n",
              "      <td>2.244117e-04</td>\n",
              "      <td>8.703281e-03</td>\n",
              "      <td>2.318583e-08</td>\n",
              "      <td>2.566181e-06</td>\n",
              "      <td>1.406309e-07</td>\n",
              "      <td>...</td>\n",
              "      <td>1.628169e-07</td>\n",
              "      <td>1.751930e-04</td>\n",
              "      <td>3.949702e-07</td>\n",
              "      <td>1.695114e-06</td>\n",
              "      <td>2.103089e-05</td>\n",
              "      <td>1.421553e-07</td>\n",
              "      <td>9.266240e-07</td>\n",
              "      <td>1.201519e-10</td>\n",
              "      <td>3.766416e-07</td>\n",
              "      <td>1.205821e-07</td>\n",
              "      <td>3.630885e-07</td>\n",
              "      <td>7.380707e-09</td>\n",
              "      <td>2.202016e-09</td>\n",
              "      <td>2.546955e-06</td>\n",
              "      <td>1.077416e-05</td>\n",
              "      <td>3.080557e-07</td>\n",
              "      <td>1.648487e-10</td>\n",
              "      <td>1.609588e-07</td>\n",
              "      <td>6.963325e-10</td>\n",
              "      <td>2.580629e-08</td>\n",
              "      <td>8.848936e-10</td>\n",
              "      <td>1.493968e-09</td>\n",
              "      <td>1.484786e-10</td>\n",
              "      <td>2.035453e-11</td>\n",
              "      <td>5.132905e-11</td>\n",
              "      <td>3.750388e-12</td>\n",
              "      <td>7.792544e-10</td>\n",
              "      <td>2.151544e-05</td>\n",
              "      <td>3.982144e-08</td>\n",
              "      <td>1.365577e-04</td>\n",
              "      <td>4.495826e-04</td>\n",
              "      <td>1.804637e-09</td>\n",
              "      <td>9.132145e-09</td>\n",
              "      <td>2.905776e-08</td>\n",
              "      <td>5.079634e-08</td>\n",
              "      <td>4.168095e-11</td>\n",
              "      <td>2.278268e-07</td>\n",
              "      <td>2.192059e-03</td>\n",
              "      <td>6.686891e-06</td>\n",
              "      <td>0.000273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100.jpg</td>\n",
              "      <td>1.746551e-16</td>\n",
              "      <td>5.582368e-09</td>\n",
              "      <td>5.886141e-09</td>\n",
              "      <td>2.716617e-10</td>\n",
              "      <td>4.974569e-15</td>\n",
              "      <td>4.126678e-15</td>\n",
              "      <td>6.010184e-11</td>\n",
              "      <td>1.512276e-11</td>\n",
              "      <td>3.648756e-12</td>\n",
              "      <td>1.773821e-14</td>\n",
              "      <td>6.821549e-11</td>\n",
              "      <td>1.347700e-08</td>\n",
              "      <td>1.633971e-12</td>\n",
              "      <td>3.923180e-16</td>\n",
              "      <td>6.165707e-13</td>\n",
              "      <td>3.563972e-10</td>\n",
              "      <td>4.747838e-13</td>\n",
              "      <td>6.830544e-11</td>\n",
              "      <td>9.843926e-19</td>\n",
              "      <td>3.002451e-13</td>\n",
              "      <td>8.058853e-15</td>\n",
              "      <td>8.791187e-17</td>\n",
              "      <td>1.233866e-08</td>\n",
              "      <td>1.091501e-16</td>\n",
              "      <td>2.384437e-11</td>\n",
              "      <td>3.200693e-11</td>\n",
              "      <td>6.901035e-14</td>\n",
              "      <td>1.593036e-09</td>\n",
              "      <td>1.766848e-10</td>\n",
              "      <td>1.824911e-09</td>\n",
              "      <td>6.159375e-11</td>\n",
              "      <td>2.658931e-13</td>\n",
              "      <td>4.047875e-14</td>\n",
              "      <td>3.872739e-15</td>\n",
              "      <td>1.469613e-10</td>\n",
              "      <td>1.837846e-12</td>\n",
              "      <td>1.982460e-09</td>\n",
              "      <td>3.817529e-13</td>\n",
              "      <td>1.006712e-12</td>\n",
              "      <td>...</td>\n",
              "      <td>1.817881e-08</td>\n",
              "      <td>1.740178e-07</td>\n",
              "      <td>5.223413e-09</td>\n",
              "      <td>4.547618e-07</td>\n",
              "      <td>4.355244e-06</td>\n",
              "      <td>1.238720e-13</td>\n",
              "      <td>1.328849e-11</td>\n",
              "      <td>2.950942e-13</td>\n",
              "      <td>2.899377e-15</td>\n",
              "      <td>6.045765e-10</td>\n",
              "      <td>3.609507e-07</td>\n",
              "      <td>2.202292e-15</td>\n",
              "      <td>1.305910e-11</td>\n",
              "      <td>2.512155e-13</td>\n",
              "      <td>2.823269e-11</td>\n",
              "      <td>4.511054e-11</td>\n",
              "      <td>1.817238e-11</td>\n",
              "      <td>1.978543e-11</td>\n",
              "      <td>5.477085e-09</td>\n",
              "      <td>3.631530e-08</td>\n",
              "      <td>2.449079e-06</td>\n",
              "      <td>2.063439e-09</td>\n",
              "      <td>2.583698e-11</td>\n",
              "      <td>3.525340e-17</td>\n",
              "      <td>1.540264e-09</td>\n",
              "      <td>3.710384e-16</td>\n",
              "      <td>5.481766e-15</td>\n",
              "      <td>3.813298e-13</td>\n",
              "      <td>8.414756e-14</td>\n",
              "      <td>4.029574e-10</td>\n",
              "      <td>1.209827e-10</td>\n",
              "      <td>5.073857e-14</td>\n",
              "      <td>4.219401e-10</td>\n",
              "      <td>1.687304e-09</td>\n",
              "      <td>2.342913e-12</td>\n",
              "      <td>1.140942e-15</td>\n",
              "      <td>8.072050e-11</td>\n",
              "      <td>2.853269e-07</td>\n",
              "      <td>8.064503e-11</td>\n",
              "      <td>0.000015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000.jpg</td>\n",
              "      <td>1.883313e-09</td>\n",
              "      <td>4.038513e-10</td>\n",
              "      <td>6.658024e-08</td>\n",
              "      <td>6.352201e-05</td>\n",
              "      <td>9.442179e-06</td>\n",
              "      <td>7.304430e-06</td>\n",
              "      <td>2.726992e-06</td>\n",
              "      <td>3.539279e-07</td>\n",
              "      <td>8.782307e-10</td>\n",
              "      <td>6.347033e-11</td>\n",
              "      <td>2.439334e-07</td>\n",
              "      <td>5.802289e-05</td>\n",
              "      <td>2.465323e-07</td>\n",
              "      <td>5.869769e-11</td>\n",
              "      <td>7.790990e-03</td>\n",
              "      <td>1.219639e-03</td>\n",
              "      <td>2.032347e-02</td>\n",
              "      <td>5.467523e-04</td>\n",
              "      <td>9.757932e-03</td>\n",
              "      <td>1.283010e-02</td>\n",
              "      <td>2.520225e-04</td>\n",
              "      <td>1.119448e-04</td>\n",
              "      <td>8.546314e-01</td>\n",
              "      <td>1.831744e-10</td>\n",
              "      <td>6.154424e-02</td>\n",
              "      <td>1.041496e-02</td>\n",
              "      <td>4.548632e-04</td>\n",
              "      <td>3.629853e-03</td>\n",
              "      <td>1.981598e-11</td>\n",
              "      <td>2.478058e-11</td>\n",
              "      <td>4.480881e-08</td>\n",
              "      <td>1.541134e-10</td>\n",
              "      <td>2.035004e-05</td>\n",
              "      <td>4.212743e-04</td>\n",
              "      <td>5.137675e-04</td>\n",
              "      <td>9.512090e-03</td>\n",
              "      <td>6.275327e-08</td>\n",
              "      <td>1.450501e-07</td>\n",
              "      <td>2.664412e-10</td>\n",
              "      <td>...</td>\n",
              "      <td>5.401142e-08</td>\n",
              "      <td>1.291029e-05</td>\n",
              "      <td>9.219033e-08</td>\n",
              "      <td>3.045943e-06</td>\n",
              "      <td>2.633215e-05</td>\n",
              "      <td>1.110018e-07</td>\n",
              "      <td>2.400370e-05</td>\n",
              "      <td>6.279826e-07</td>\n",
              "      <td>2.285331e-05</td>\n",
              "      <td>2.140738e-07</td>\n",
              "      <td>6.243956e-08</td>\n",
              "      <td>5.989051e-06</td>\n",
              "      <td>9.679781e-09</td>\n",
              "      <td>2.623280e-04</td>\n",
              "      <td>2.926125e-04</td>\n",
              "      <td>4.064123e-07</td>\n",
              "      <td>1.754597e-11</td>\n",
              "      <td>8.124137e-07</td>\n",
              "      <td>3.764302e-09</td>\n",
              "      <td>3.213538e-08</td>\n",
              "      <td>9.841741e-10</td>\n",
              "      <td>1.456208e-08</td>\n",
              "      <td>7.035767e-10</td>\n",
              "      <td>1.510454e-12</td>\n",
              "      <td>2.393509e-12</td>\n",
              "      <td>1.639587e-09</td>\n",
              "      <td>9.412394e-11</td>\n",
              "      <td>1.069726e-06</td>\n",
              "      <td>1.253851e-09</td>\n",
              "      <td>2.663621e-06</td>\n",
              "      <td>3.669021e-05</td>\n",
              "      <td>2.611278e-09</td>\n",
              "      <td>4.347782e-09</td>\n",
              "      <td>4.751151e-08</td>\n",
              "      <td>1.831700e-08</td>\n",
              "      <td>6.238722e-10</td>\n",
              "      <td>6.527649e-07</td>\n",
              "      <td>4.768445e-03</td>\n",
              "      <td>2.270616e-05</td>\n",
              "      <td>0.000222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10000.jpg</td>\n",
              "      <td>1.369679e-08</td>\n",
              "      <td>4.364478e-10</td>\n",
              "      <td>5.548906e-07</td>\n",
              "      <td>4.219160e-07</td>\n",
              "      <td>2.058264e-13</td>\n",
              "      <td>9.908187e-10</td>\n",
              "      <td>2.341904e-11</td>\n",
              "      <td>1.200551e-10</td>\n",
              "      <td>4.318691e-07</td>\n",
              "      <td>1.103372e-10</td>\n",
              "      <td>1.062647e-09</td>\n",
              "      <td>5.986603e-08</td>\n",
              "      <td>6.435817e-10</td>\n",
              "      <td>3.063238e-09</td>\n",
              "      <td>1.098226e-09</td>\n",
              "      <td>1.048864e-12</td>\n",
              "      <td>2.134770e-09</td>\n",
              "      <td>9.576838e-13</td>\n",
              "      <td>4.275705e-08</td>\n",
              "      <td>2.301034e-09</td>\n",
              "      <td>3.682928e-11</td>\n",
              "      <td>1.068818e-17</td>\n",
              "      <td>1.148504e-09</td>\n",
              "      <td>1.202638e-07</td>\n",
              "      <td>8.219785e-11</td>\n",
              "      <td>3.277965e-10</td>\n",
              "      <td>1.912161e-08</td>\n",
              "      <td>7.464523e-06</td>\n",
              "      <td>4.959040e-15</td>\n",
              "      <td>2.383567e-13</td>\n",
              "      <td>9.540740e-13</td>\n",
              "      <td>7.726746e-12</td>\n",
              "      <td>1.099293e-10</td>\n",
              "      <td>2.090166e-02</td>\n",
              "      <td>1.645584e-05</td>\n",
              "      <td>6.576155e-04</td>\n",
              "      <td>2.298893e-09</td>\n",
              "      <td>7.518709e-08</td>\n",
              "      <td>4.649217e-16</td>\n",
              "      <td>...</td>\n",
              "      <td>9.743608e-07</td>\n",
              "      <td>2.710769e-03</td>\n",
              "      <td>3.723174e-07</td>\n",
              "      <td>1.582252e-07</td>\n",
              "      <td>2.073547e-04</td>\n",
              "      <td>4.120300e-06</td>\n",
              "      <td>6.314191e-10</td>\n",
              "      <td>4.663646e-17</td>\n",
              "      <td>3.709406e-11</td>\n",
              "      <td>2.151886e-06</td>\n",
              "      <td>1.501039e-06</td>\n",
              "      <td>2.781765e-10</td>\n",
              "      <td>2.232823e-15</td>\n",
              "      <td>4.506999e-08</td>\n",
              "      <td>1.244510e-10</td>\n",
              "      <td>8.910384e-11</td>\n",
              "      <td>4.330175e-16</td>\n",
              "      <td>1.400863e-10</td>\n",
              "      <td>1.027704e-16</td>\n",
              "      <td>1.182289e-14</td>\n",
              "      <td>9.022739e-13</td>\n",
              "      <td>2.509027e-12</td>\n",
              "      <td>3.196290e-14</td>\n",
              "      <td>7.543462e-17</td>\n",
              "      <td>1.101601e-10</td>\n",
              "      <td>4.401335e-11</td>\n",
              "      <td>1.623569e-07</td>\n",
              "      <td>7.667762e-06</td>\n",
              "      <td>1.816621e-03</td>\n",
              "      <td>9.723578e-01</td>\n",
              "      <td>3.086601e-05</td>\n",
              "      <td>1.121332e-10</td>\n",
              "      <td>1.400375e-08</td>\n",
              "      <td>1.160203e-05</td>\n",
              "      <td>1.511618e-08</td>\n",
              "      <td>9.069117e-10</td>\n",
              "      <td>1.020763e-10</td>\n",
              "      <td>1.213799e-03</td>\n",
              "      <td>4.358135e-07</td>\n",
              "      <td>0.000033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130395</th>\n",
              "      <td>99994.jpg</td>\n",
              "      <td>1.967893e-14</td>\n",
              "      <td>8.735568e-13</td>\n",
              "      <td>4.100304e-11</td>\n",
              "      <td>4.909427e-08</td>\n",
              "      <td>2.487255e-10</td>\n",
              "      <td>3.755800e-08</td>\n",
              "      <td>3.393592e-07</td>\n",
              "      <td>2.045813e-05</td>\n",
              "      <td>7.404189e-11</td>\n",
              "      <td>6.646522e-13</td>\n",
              "      <td>2.057961e-02</td>\n",
              "      <td>8.836482e-01</td>\n",
              "      <td>9.411298e-02</td>\n",
              "      <td>5.893486e-08</td>\n",
              "      <td>2.415855e-10</td>\n",
              "      <td>1.439325e-08</td>\n",
              "      <td>2.727384e-10</td>\n",
              "      <td>1.133057e-09</td>\n",
              "      <td>9.193279e-12</td>\n",
              "      <td>4.070445e-09</td>\n",
              "      <td>3.373016e-11</td>\n",
              "      <td>2.673887e-10</td>\n",
              "      <td>5.723027e-08</td>\n",
              "      <td>7.038908e-10</td>\n",
              "      <td>1.583278e-09</td>\n",
              "      <td>1.297792e-09</td>\n",
              "      <td>2.019582e-08</td>\n",
              "      <td>7.401739e-06</td>\n",
              "      <td>2.903400e-08</td>\n",
              "      <td>6.049400e-12</td>\n",
              "      <td>1.503036e-08</td>\n",
              "      <td>1.666323e-10</td>\n",
              "      <td>6.111429e-07</td>\n",
              "      <td>1.288304e-08</td>\n",
              "      <td>8.580750e-05</td>\n",
              "      <td>3.451557e-08</td>\n",
              "      <td>4.092132e-06</td>\n",
              "      <td>1.077808e-04</td>\n",
              "      <td>2.245115e-10</td>\n",
              "      <td>...</td>\n",
              "      <td>9.473589e-05</td>\n",
              "      <td>3.275275e-11</td>\n",
              "      <td>5.558507e-10</td>\n",
              "      <td>2.793558e-08</td>\n",
              "      <td>1.485832e-07</td>\n",
              "      <td>5.988422e-11</td>\n",
              "      <td>4.497263e-07</td>\n",
              "      <td>3.507520e-09</td>\n",
              "      <td>7.294314e-04</td>\n",
              "      <td>2.264792e-07</td>\n",
              "      <td>7.761977e-08</td>\n",
              "      <td>1.881668e-07</td>\n",
              "      <td>2.359623e-05</td>\n",
              "      <td>2.183826e-06</td>\n",
              "      <td>3.746761e-07</td>\n",
              "      <td>2.459592e-09</td>\n",
              "      <td>3.211864e-07</td>\n",
              "      <td>2.028473e-05</td>\n",
              "      <td>7.177927e-09</td>\n",
              "      <td>1.673815e-08</td>\n",
              "      <td>2.385002e-09</td>\n",
              "      <td>1.046061e-06</td>\n",
              "      <td>2.102705e-09</td>\n",
              "      <td>9.949687e-11</td>\n",
              "      <td>1.219702e-08</td>\n",
              "      <td>6.585514e-09</td>\n",
              "      <td>1.378973e-09</td>\n",
              "      <td>3.481595e-08</td>\n",
              "      <td>3.171099e-11</td>\n",
              "      <td>2.113629e-09</td>\n",
              "      <td>3.138896e-05</td>\n",
              "      <td>1.245439e-10</td>\n",
              "      <td>1.385882e-05</td>\n",
              "      <td>2.727331e-09</td>\n",
              "      <td>9.419988e-10</td>\n",
              "      <td>2.976864e-08</td>\n",
              "      <td>2.430165e-07</td>\n",
              "      <td>2.781703e-07</td>\n",
              "      <td>6.356102e-05</td>\n",
              "      <td>0.000088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130396</th>\n",
              "      <td>99995.jpg</td>\n",
              "      <td>5.381751e-10</td>\n",
              "      <td>6.274540e-10</td>\n",
              "      <td>4.790215e-07</td>\n",
              "      <td>2.769299e-06</td>\n",
              "      <td>2.491563e-06</td>\n",
              "      <td>8.943557e-05</td>\n",
              "      <td>3.075157e-04</td>\n",
              "      <td>4.004919e-03</td>\n",
              "      <td>8.656660e-08</td>\n",
              "      <td>4.556654e-07</td>\n",
              "      <td>4.536867e-05</td>\n",
              "      <td>1.581206e-03</td>\n",
              "      <td>1.964744e-05</td>\n",
              "      <td>6.231250e-09</td>\n",
              "      <td>4.015258e-03</td>\n",
              "      <td>5.803840e-07</td>\n",
              "      <td>4.389624e-04</td>\n",
              "      <td>1.292956e-07</td>\n",
              "      <td>7.003086e-07</td>\n",
              "      <td>3.058155e-07</td>\n",
              "      <td>7.915556e-07</td>\n",
              "      <td>5.562005e-08</td>\n",
              "      <td>6.383316e-05</td>\n",
              "      <td>9.616475e-10</td>\n",
              "      <td>2.149953e-05</td>\n",
              "      <td>4.321692e-06</td>\n",
              "      <td>4.049133e-06</td>\n",
              "      <td>4.504777e-04</td>\n",
              "      <td>2.132992e-09</td>\n",
              "      <td>5.001547e-09</td>\n",
              "      <td>1.861942e-08</td>\n",
              "      <td>3.768386e-13</td>\n",
              "      <td>1.542533e-05</td>\n",
              "      <td>9.318331e-03</td>\n",
              "      <td>2.614428e-02</td>\n",
              "      <td>6.367951e-03</td>\n",
              "      <td>1.729240e-04</td>\n",
              "      <td>3.698697e-02</td>\n",
              "      <td>8.609489e-09</td>\n",
              "      <td>...</td>\n",
              "      <td>1.188337e-03</td>\n",
              "      <td>1.698797e-05</td>\n",
              "      <td>4.014438e-09</td>\n",
              "      <td>1.740474e-06</td>\n",
              "      <td>5.568245e-05</td>\n",
              "      <td>3.636577e-07</td>\n",
              "      <td>3.108588e-07</td>\n",
              "      <td>6.461865e-12</td>\n",
              "      <td>2.469111e-05</td>\n",
              "      <td>1.632495e-05</td>\n",
              "      <td>4.536671e-06</td>\n",
              "      <td>8.846438e-07</td>\n",
              "      <td>3.983625e-09</td>\n",
              "      <td>5.941028e-04</td>\n",
              "      <td>2.033225e-04</td>\n",
              "      <td>3.792260e-09</td>\n",
              "      <td>2.310879e-10</td>\n",
              "      <td>4.996132e-08</td>\n",
              "      <td>1.215146e-11</td>\n",
              "      <td>1.279981e-07</td>\n",
              "      <td>4.265391e-08</td>\n",
              "      <td>5.890634e-10</td>\n",
              "      <td>3.843474e-10</td>\n",
              "      <td>7.889361e-10</td>\n",
              "      <td>7.472312e-09</td>\n",
              "      <td>6.814155e-07</td>\n",
              "      <td>1.733423e-08</td>\n",
              "      <td>2.619618e-03</td>\n",
              "      <td>1.054470e-04</td>\n",
              "      <td>1.893557e-04</td>\n",
              "      <td>4.987567e-01</td>\n",
              "      <td>2.032451e-10</td>\n",
              "      <td>4.289702e-08</td>\n",
              "      <td>3.988319e-07</td>\n",
              "      <td>4.493298e-08</td>\n",
              "      <td>1.971848e-07</td>\n",
              "      <td>7.854092e-08</td>\n",
              "      <td>5.449409e-03</td>\n",
              "      <td>2.368094e-01</td>\n",
              "      <td>0.000182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130397</th>\n",
              "      <td>99996.jpg</td>\n",
              "      <td>8.502930e-12</td>\n",
              "      <td>5.243981e-12</td>\n",
              "      <td>4.248429e-10</td>\n",
              "      <td>4.403753e-06</td>\n",
              "      <td>3.913661e-10</td>\n",
              "      <td>2.293224e-06</td>\n",
              "      <td>1.360557e-05</td>\n",
              "      <td>8.185710e-04</td>\n",
              "      <td>3.158531e-09</td>\n",
              "      <td>9.881457e-11</td>\n",
              "      <td>6.000718e-02</td>\n",
              "      <td>6.888365e-01</td>\n",
              "      <td>4.346124e-02</td>\n",
              "      <td>3.078955e-04</td>\n",
              "      <td>7.967241e-08</td>\n",
              "      <td>1.762081e-10</td>\n",
              "      <td>3.144441e-10</td>\n",
              "      <td>2.254604e-11</td>\n",
              "      <td>6.401158e-10</td>\n",
              "      <td>4.793959e-09</td>\n",
              "      <td>1.620507e-12</td>\n",
              "      <td>3.347965e-13</td>\n",
              "      <td>6.551198e-09</td>\n",
              "      <td>2.181118e-08</td>\n",
              "      <td>5.767109e-10</td>\n",
              "      <td>2.684051e-10</td>\n",
              "      <td>5.572135e-08</td>\n",
              "      <td>1.882808e-04</td>\n",
              "      <td>7.516564e-07</td>\n",
              "      <td>3.231121e-11</td>\n",
              "      <td>3.043401e-08</td>\n",
              "      <td>6.464534e-11</td>\n",
              "      <td>2.676137e-05</td>\n",
              "      <td>3.982272e-05</td>\n",
              "      <td>1.100826e-03</td>\n",
              "      <td>1.060240e-06</td>\n",
              "      <td>7.454026e-06</td>\n",
              "      <td>5.193312e-03</td>\n",
              "      <td>2.107757e-11</td>\n",
              "      <td>...</td>\n",
              "      <td>3.156234e-04</td>\n",
              "      <td>3.063307e-08</td>\n",
              "      <td>1.512894e-10</td>\n",
              "      <td>8.047869e-08</td>\n",
              "      <td>4.829088e-06</td>\n",
              "      <td>1.307990e-09</td>\n",
              "      <td>4.601709e-07</td>\n",
              "      <td>6.007833e-10</td>\n",
              "      <td>1.029894e-04</td>\n",
              "      <td>3.101506e-05</td>\n",
              "      <td>1.236316e-06</td>\n",
              "      <td>1.545744e-07</td>\n",
              "      <td>7.742991e-08</td>\n",
              "      <td>1.090079e-05</td>\n",
              "      <td>3.211205e-05</td>\n",
              "      <td>1.377985e-09</td>\n",
              "      <td>6.888749e-08</td>\n",
              "      <td>3.864031e-07</td>\n",
              "      <td>6.347571e-12</td>\n",
              "      <td>1.844456e-10</td>\n",
              "      <td>5.562198e-10</td>\n",
              "      <td>1.720457e-07</td>\n",
              "      <td>1.916399e-08</td>\n",
              "      <td>1.044460e-10</td>\n",
              "      <td>7.275215e-08</td>\n",
              "      <td>8.226089e-07</td>\n",
              "      <td>1.293184e-07</td>\n",
              "      <td>3.046209e-07</td>\n",
              "      <td>6.458007e-07</td>\n",
              "      <td>1.402917e-07</td>\n",
              "      <td>4.268861e-03</td>\n",
              "      <td>3.287269e-10</td>\n",
              "      <td>6.265843e-05</td>\n",
              "      <td>1.672902e-07</td>\n",
              "      <td>1.794534e-07</td>\n",
              "      <td>1.975124e-04</td>\n",
              "      <td>1.234396e-06</td>\n",
              "      <td>2.964621e-04</td>\n",
              "      <td>2.601932e-03</td>\n",
              "      <td>0.000758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130398</th>\n",
              "      <td>99997.jpg</td>\n",
              "      <td>4.572866e-05</td>\n",
              "      <td>3.590241e-07</td>\n",
              "      <td>9.906559e-03</td>\n",
              "      <td>8.535540e-08</td>\n",
              "      <td>4.170911e-07</td>\n",
              "      <td>3.037210e-01</td>\n",
              "      <td>6.673383e-03</td>\n",
              "      <td>5.148952e-04</td>\n",
              "      <td>2.109226e-05</td>\n",
              "      <td>1.706443e-04</td>\n",
              "      <td>3.438721e-08</td>\n",
              "      <td>9.792061e-05</td>\n",
              "      <td>4.924638e-09</td>\n",
              "      <td>4.517575e-10</td>\n",
              "      <td>1.022358e-05</td>\n",
              "      <td>3.739358e-07</td>\n",
              "      <td>3.680651e-08</td>\n",
              "      <td>1.408821e-06</td>\n",
              "      <td>9.200026e-08</td>\n",
              "      <td>1.680030e-08</td>\n",
              "      <td>2.658046e-06</td>\n",
              "      <td>7.640767e-08</td>\n",
              "      <td>1.038467e-04</td>\n",
              "      <td>3.868815e-10</td>\n",
              "      <td>2.343706e-03</td>\n",
              "      <td>3.668938e-05</td>\n",
              "      <td>5.283259e-07</td>\n",
              "      <td>1.907881e-05</td>\n",
              "      <td>1.730020e-07</td>\n",
              "      <td>5.284610e-05</td>\n",
              "      <td>5.900064e-05</td>\n",
              "      <td>8.370622e-09</td>\n",
              "      <td>1.562377e-08</td>\n",
              "      <td>2.125121e-03</td>\n",
              "      <td>6.646178e-04</td>\n",
              "      <td>7.619237e-02</td>\n",
              "      <td>1.932530e-05</td>\n",
              "      <td>6.304933e-05</td>\n",
              "      <td>1.411512e-08</td>\n",
              "      <td>...</td>\n",
              "      <td>1.724668e-04</td>\n",
              "      <td>2.384097e-02</td>\n",
              "      <td>1.270109e-05</td>\n",
              "      <td>8.144185e-03</td>\n",
              "      <td>1.929132e-01</td>\n",
              "      <td>1.339182e-06</td>\n",
              "      <td>2.430650e-06</td>\n",
              "      <td>4.203136e-10</td>\n",
              "      <td>2.348271e-12</td>\n",
              "      <td>1.535251e-01</td>\n",
              "      <td>3.639564e-02</td>\n",
              "      <td>1.786175e-08</td>\n",
              "      <td>1.726101e-07</td>\n",
              "      <td>3.726079e-08</td>\n",
              "      <td>1.252230e-07</td>\n",
              "      <td>4.130128e-07</td>\n",
              "      <td>3.284919e-09</td>\n",
              "      <td>4.308979e-06</td>\n",
              "      <td>4.583209e-09</td>\n",
              "      <td>6.562559e-06</td>\n",
              "      <td>2.372131e-06</td>\n",
              "      <td>2.935520e-07</td>\n",
              "      <td>6.252262e-08</td>\n",
              "      <td>7.878296e-08</td>\n",
              "      <td>1.213443e-03</td>\n",
              "      <td>4.645005e-10</td>\n",
              "      <td>6.547317e-07</td>\n",
              "      <td>3.671115e-07</td>\n",
              "      <td>3.320500e-08</td>\n",
              "      <td>9.419698e-06</td>\n",
              "      <td>8.905685e-07</td>\n",
              "      <td>4.129678e-03</td>\n",
              "      <td>1.241242e-05</td>\n",
              "      <td>1.398630e-02</td>\n",
              "      <td>8.191168e-08</td>\n",
              "      <td>1.709085e-06</td>\n",
              "      <td>8.638604e-04</td>\n",
              "      <td>1.247365e-02</td>\n",
              "      <td>1.430968e-04</td>\n",
              "      <td>0.073234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130399</th>\n",
              "      <td>99999.jpg</td>\n",
              "      <td>2.468391e-10</td>\n",
              "      <td>4.662440e-10</td>\n",
              "      <td>9.997759e-07</td>\n",
              "      <td>3.152744e-08</td>\n",
              "      <td>1.378678e-06</td>\n",
              "      <td>2.377373e-05</td>\n",
              "      <td>1.718205e-05</td>\n",
              "      <td>9.588913e-06</td>\n",
              "      <td>1.362586e-11</td>\n",
              "      <td>1.624355e-09</td>\n",
              "      <td>4.663537e-09</td>\n",
              "      <td>1.888981e-06</td>\n",
              "      <td>3.043528e-09</td>\n",
              "      <td>6.143434e-12</td>\n",
              "      <td>1.787183e-08</td>\n",
              "      <td>1.165799e-06</td>\n",
              "      <td>3.031348e-10</td>\n",
              "      <td>1.783935e-08</td>\n",
              "      <td>8.205927e-09</td>\n",
              "      <td>1.950169e-08</td>\n",
              "      <td>3.659400e-08</td>\n",
              "      <td>1.255707e-05</td>\n",
              "      <td>7.756594e-06</td>\n",
              "      <td>3.908812e-12</td>\n",
              "      <td>3.302817e-05</td>\n",
              "      <td>6.827711e-08</td>\n",
              "      <td>3.437222e-05</td>\n",
              "      <td>4.613881e-07</td>\n",
              "      <td>1.596029e-07</td>\n",
              "      <td>2.306159e-07</td>\n",
              "      <td>7.019935e-05</td>\n",
              "      <td>2.697562e-09</td>\n",
              "      <td>4.193078e-10</td>\n",
              "      <td>2.123690e-09</td>\n",
              "      <td>1.313624e-06</td>\n",
              "      <td>2.315972e-07</td>\n",
              "      <td>1.490433e-06</td>\n",
              "      <td>2.665214e-06</td>\n",
              "      <td>3.179252e-07</td>\n",
              "      <td>...</td>\n",
              "      <td>2.236520e-07</td>\n",
              "      <td>1.571677e-07</td>\n",
              "      <td>1.430074e-08</td>\n",
              "      <td>6.462942e-07</td>\n",
              "      <td>3.937260e-06</td>\n",
              "      <td>1.276594e-09</td>\n",
              "      <td>3.530123e-04</td>\n",
              "      <td>4.476504e-06</td>\n",
              "      <td>3.703485e-08</td>\n",
              "      <td>3.783363e-07</td>\n",
              "      <td>4.766188e-06</td>\n",
              "      <td>3.730348e-09</td>\n",
              "      <td>1.445488e-04</td>\n",
              "      <td>2.177585e-09</td>\n",
              "      <td>2.921699e-06</td>\n",
              "      <td>4.658808e-08</td>\n",
              "      <td>8.768228e-07</td>\n",
              "      <td>2.975181e-05</td>\n",
              "      <td>3.471324e-05</td>\n",
              "      <td>1.495424e-03</td>\n",
              "      <td>2.278667e-06</td>\n",
              "      <td>2.260172e-07</td>\n",
              "      <td>1.805810e-09</td>\n",
              "      <td>2.329614e-06</td>\n",
              "      <td>2.014526e-06</td>\n",
              "      <td>5.606557e-11</td>\n",
              "      <td>1.162798e-10</td>\n",
              "      <td>1.318890e-08</td>\n",
              "      <td>1.110649e-11</td>\n",
              "      <td>6.610324e-09</td>\n",
              "      <td>1.430959e-07</td>\n",
              "      <td>1.236227e-07</td>\n",
              "      <td>4.352242e-06</td>\n",
              "      <td>2.505182e-09</td>\n",
              "      <td>8.707912e-09</td>\n",
              "      <td>2.351630e-11</td>\n",
              "      <td>3.479226e-07</td>\n",
              "      <td>1.131826e-05</td>\n",
              "      <td>6.088884e-05</td>\n",
              "      <td>0.001155</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>130400 rows  122 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            image  ...  unknown_unclassified\n",
              "0           1.jpg  ...              0.001341\n",
              "1          10.jpg  ...              0.000273\n",
              "2         100.jpg  ...              0.000015\n",
              "3        1000.jpg  ...              0.000222\n",
              "4       10000.jpg  ...              0.000033\n",
              "...           ...  ...                   ...\n",
              "130395  99994.jpg  ...              0.000088\n",
              "130396  99995.jpg  ...              0.000182\n",
              "130397  99996.jpg  ...              0.000758\n",
              "130398  99997.jpg  ...              0.073234\n",
              "130399  99999.jpg  ...              0.001155\n",
              "\n",
              "[130400 rows x 122 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6IQIehhR0jJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission_csv.to_csv(os.path.join(csv_path, 'sub.csv'), index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INSwqYKaf3CU",
        "colab_type": "code",
        "outputId": "65964eab-aaa1-46b9-a366-4c312570b494",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!kaggle competitions submit datasciencebowl -f sub.csv -m KFold-aug"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "100% 336M/336M [00:10<00:00, 34.8MB/s]\n",
            "Successfully submitted to National Data Science Bowl"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFEMT0R8hGEe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kaggle competitions submissions datasciencebowls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLfOOY8aPf-a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}